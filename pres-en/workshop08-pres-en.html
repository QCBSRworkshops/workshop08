<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Workshop 8: Generalized additive models</title>
    <meta charset="utf-8" />
    <meta name="author" content="Québec Centre for Biodiversity Science" />
    <link href="assets/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css">
    <link rel="stylesheet" href="qcbsR.css" type="text/css" />
    <link rel="stylesheet" href="qcbsR-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Workshop 8: Generalized additive models
## QCBS R Workshop Series
### Québec Centre for Biodiversity Science

---







class: inverse, center, middle



# About this workshop
[![badge](https://img.shields.io/static/v1?style=for-the-badge&amp;label=repo&amp;message=dev&amp;color=6f42c1&amp;logo=github)](https://github.com/QCBSRworkshops/workshop08)
[![badge](https://img.shields.io/static/v1?style=for-the-badge&amp;label=wiki&amp;message=08&amp;logo=wikipedia)](https://wiki.qcbs.ca/r_workshop8)
[![badge](https://img.shields.io/static/v1?style=for-the-badge&amp;label=Slides&amp;message=08&amp;color=red&amp;logo=html5)](https://qcbsrworkshops.github.io/workshop08/workshop08-en/workshop08-en.html)
[![badge](https://img.shields.io/static/v1?style=for-the-badge&amp;label=Slides&amp;message=08&amp;color=red&amp;logo=adobe-acrobat-reader)](https://qcbsrworkshops.github.io/workshop08/workshop08-en/workshop08-en.pdf)
[![badge](https://img.shields.io/static/v1?style=for-the-badge&amp;label=script&amp;message=08&amp;color=2a50b8&amp;logo=r)](https://qcbsrworkshops.github.io/workshop08/workshop08-en/workshop08-en.R)

---

# Required packages

* [ggplot2](https://cran.r-project.org/package=ggplot2)
* [itsadug](https://cran.r-project.org/package=itsadug)
* [mgcv](https://cran.r-project.org/package=mgcv)

&lt;br&gt;

```R
install.packages(c('ggplot2', 'itsadug', 'mgcv'))
```
---

# Workshop overview

1. The linear model... and where it fails
2. Introduction to GAM
3. GAM with multiple smooth terms
4. Interactions
5. Generalization of the additive model
6. Changing basis
7. Quick intro to GAMM

---

# Learning objectives

1. Use the mgcv package to fit non-linear relationships,
2. Understand the output of a GAM to help you understand your data,
3. Use tests to determine if a non-linear model fits better than a linear one,
4. Include smooth interactions between variables,
5. Understand the idea of a basis function, and why it makes GAMs so powerful,
6. Account for dependence in data (autocorrelation, hierarchical structure) using GAMMs.

---
# Prerequisites

&gt; Some experience in R (enough to be able to run a script and examine data and R objects)
&gt; a basic knowledge of linear regression.

---
class: inverse, center, middle
# 1. The linear model

## ...and where it fails

---
# Linear regression

Regression is the workhorse of statistics. It allows us to model a response variable as a function of predictors plus error.

--

As we saw in the [linear models workshop](http://qcbs.ca/wiki/r_workshop4), a linear model is based on 4 major assumptions:

1. Linear relationship between response and predictor variables: `$$y_i = \beta_0 + \beta_1 \times x_i + \epsilon_i$$`
2. Normally distributed error: `$$\epsilon_i \sim \mathcal{N}(0,\,\sigma^2)$$`
3. Homogeneity of the variance
4. Independance of the errors

&lt;br&gt;

--

*Linear model with multiple predictors:*

`$$y_i = \beta_0 + \beta_1x_{1,i}+\beta_2x_{2,i}+\beta_3x_{3,i}+...+\beta_kx_{k,i} + \epsilon_i$$`

???
- A linear model can easily accomodate certain types of non-linear responses (e.g. `\(x^2\)`) but this approach strongly relies on (arbitrary or well-informed) choices, and is less flexible than using an additive model.

---
# Linear regression

There's only one way for the linear model to be right:

.center[
![](images/linreg.png)
]


---
# Linear regression

And yet so many ways for it to fail:

.center[
![:scale 60%](images/linreg_bad.png)
]

---
# Linear regression

**What's the problem and do we fix it?**

A **linear model** tries to fit the best **straight line** that passes through the data, so it doesn't work well for all datasets.

In contrast, a **GAM** can capture complexe relationships by fitting a **non-linear smooth function** through the data, while controlling how wiggly the smooth can get (more on this later).

---
class: inverse, center, middle

## 2. Introduction to GAM

---
# Generalized Additive Models (GAM)

Let's look at an example. First, we will load the `ISIT` datasets.


```r
isit &lt;- read.csv("data/ISIT.csv")
head(isit)
```

This datasets is comprised of bioluminescence levels (`Sources`) in relation to depth, seasons and different stations.


For now, let's focus on Season 2.



```r
isit2 &lt;- subset(isit, Season==2)
```

---
# GAM

Trying to fit `Sources` to `SampleDepth` as a linear regression model, we would violate the assumptions listed above.

```r
linear_model &lt;- gam(Sources ~ SampleDepth, data = isit2)
data_plot &lt;- ggplot(isit2, aes(y = Sources, x = SampleDepth)) + geom_point() +
             geom_line(colour = "red", size = 1.2,aes(y = fitted(linear_model))) + theme_bw()
data_plot
```

&lt;img src="workshop08-pres-en_files/figure-html/unnamed-chunk-5-1.png" width="432" style="display: block; margin: auto;" /&gt;

---
exclude:true
# Generalized Additive Models (GAM)

Let's look at an example. First, we'll generate some data, and plot it.


```r
library(ggplot2)
set.seed(10)
n &lt;- 250
x &lt;- runif(n,0,5)
y_model &lt;- 3*x/(1+2*x)
y_obs &lt;- rnorm(n,y_model,0.1)
data_plot &lt;- qplot(x, y_obs) +
  geom_line(aes(y=y_model)) +
  theme_bw()
data_plot
```

---
exclude:true
# GAM



---
exclude:true
# GAM

Trying to fit these data as a linear regression model, we would violate the assumptions listed above.



---
# GAM

**Relationship between the response variable and predictor variables**

One predictor variable:
`$$y_i = \beta_0 + f(x_i) + \epsilon$$`

Multiple predictor variables:
`$$y_i = \beta_0 + f_1(x_{1,i}) + f_2(x_{2,i}) + ... + \epsilon$$`

One big advantage of using a GAM over a manual specification of the model is that the optimal shape, i.e. *the degree of smoothness* of `\(f(x)\)`, is determined automatically depending on the fitting method (usually maximum likelihood).

???

- Strictly speaking, the equations are for a Gaussian GAM with identity link, which is also called "additive model" (without "generalized").

---
#GAM

Let's try to fit a model to the data using a smooth function `s()` with `mgcv::gam()`


```r
gam_model &lt;- gam(Sources ~ s(SampleDepth), data = isit2)
```
--


```
...
# Family: gaussian 
# Link function: identity 
# 
# Parametric coefficients:
#             Estimate Std. Error t value Pr(&gt;|t|)    
# (Intercept)  12.8937     0.2471   52.17   &lt;2e-16 ***
# 
# Approximate significance of smooth terms:
#                  edf Ref.df     F p-value    
# s(SampleDepth) 8.908  8.998 214.1  &lt;2e-16 ***
# ---
# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
# 
# R-sq.(adj) =   0.81   Deviance explained = 81.4%
# GCV = 28.287  Scale est. = 27.669    n = 453
...
```

---
# GAM

&lt;img src="workshop08-pres-en_files/figure-html/unnamed-chunk-11-1.png" width="432" style="display: block; margin: auto;" /&gt;

Note: as opposed to one fixed coefficient, `\(\beta\)` in linear regression, the smooth function can continually change over the range of the predictor `\(x\)`.

---
exclude:true
# GAM

Let's try to fit the data using a smooth function `s()` with `mgcv::gam()`



```r
library(mgcv)
gam_model &lt;- gam(y_obs ~ s(x))
summary(gam_model)

data_plot &lt;- data_plot +
     geom_line(colour = "blue", size = 1.2, 
               aes(y = fitted(gam_model)))
data_plot
```

---
exclude:true
# GAM



---
exclude:true
# GAM



.comment[Note: as opposed to one fixed coefficient, \beta in linear regression, the smooth function can continually change over the range of the predictor x]


---
# GAM

The `mgcv` package also includes a default plot to look at the smooths:


```r
plot(gam_model)
```

&lt;img src="workshop08-pres-en_files/figure-html/unnamed-chunk-15-1.png" width="432" style="display: block; margin: auto;" /&gt;

---
# Test for linearity using GAM

We can use `gam()` and `AIC()` to test whether an assumption of linearity is justified. To do so, we must simply set our smoothed model so that it is nested in our linear model.


```r
linear_model &lt;- gam(Sources ~ SampleDepth, data = isit2) # fit a regular linear model using gam()
nested_gam_model &lt;- gam(Sources ~ s(SampleDepth), data = isit2)
AIC(linear_model, nested_gam_model)
#                        df      AIC
# linear_model      3.00000 3143.720
# nested_gam_model 10.90825 2801.451
```

**AIC of the nested gam is lower, so linearity is not justified**

.comment[Note that the model `y ~ s(x)` is nested in the model `y ~ x`.]

---
exclude:true
# Test for linearity using GAM

We can use `gam()` and `anova()` to test whether an assumption of linearity is justified. To do so, we must simply set our smoothed model so that it is nested in our linear model.


```r
linear_model &lt;- gam(y_obs ~ x) # fit a regular linear model using gam()
nested_gam_model &lt;- gam(y_obs ~ s(x) + x)
anova(linear_model, nested_gam_model, test = "Chisq")
```

.comment[Note that the model `y_obs~s(x)` gives exactly the same results as `y_obs~s(x)+x`. We used the s(x)+x to illustrate the nestedness of the model, but the +x can be omitted.]

---
# Challenge 1 ![:cube]()

We will now try this comparison test with the data recorded in the first season, just to get a handle on it.


```r
isit1 &lt;- subset(isit, Season==1)
```

1. Fit a linear and smoothed GAM model to the relation between `SampleDepth` and `Sources`.
2. Determine if linearity is justified for this data.
3. How many effective degrees of freedom does the smoothed term have?


&lt;!-- we didn't talk about **edf** before... --&gt;
---
# Challenge 1 - Solution ![:cube]()


```r
linear_model_s1 &lt;- gam(Sources ~ SampleDepth, data = isit1)
gam_model_s1 &lt;- gam(Sources ~ s(SampleDepth), data = isit1)
```

---
# Challenge 1 - Solution ![:cube]()

&lt;img src="workshop08-pres-en_files/figure-html/unnamed-chunk-20-1.png" width="432" style="display: block; margin: auto;" /&gt;

---
# Challenge 1 - Solution ![:cube]()


```r
linear_model_s1 &lt;- gam(Sources ~ SampleDepth,data = isit1)
nested_gam_model_s1 &lt;- gam(Sources ~ s(SampleDepth),data = isit1)

AIC(linear_model_s1, nested_gam_model_s1)
#                           df      AIC
# linear_model_s1     3.000000 2324.905
# nested_gam_model_s1 9.644938 2121.249
```


---
# Challenge 1 - Solution ![:cube]()


```r
nested_gam_model_s1
# 
# Family: gaussian 
# Link function: identity 
# 
# Formula:
# Sources ~ s(SampleDepth)
# 
# Estimated degrees of freedom:
# 7.64  total = 8.64 
# 
# GCV score: 32.13946
```

**Answer** Yes, non-linearity is justified. The effective degrees of freedom (**EDF**) are &gt;&gt; 1 (we'll get back to this soon).

---
exclude:true
# Challenge 1 ![:cube]()

We will now try this comparison test with some new simulated data, just to get a handle on it.


```r
n &lt;- 250
x_test &lt;- runif(n, -5, 5)
y_test_fit &lt;- 4 * dnorm(x_test)
y_test_obs &lt;- rnorm(n, y_test_fit, 0.2)
```

1. Fit a linear and smoothed GAM model to the relation between `x_test` and `y_test_obs`.
2. Determine if linearity is justified for this data.
3. What is the estimated degrees of freedom of the smoothed term?


&lt;!-- we didn't talk about **edf** before... --&gt;

---
exclude:true
# Challenge 1 - Solution ![:cube]()


```r
linear_model_test &lt;- gam(y_test_obs ~ x_test)
nested_gam_model_test &lt;- gam(y_test_obs ~ s(x_test) + x_test)

anova(linear_model_test, nested_gam_model_test, test="Chisq")
```

---
exclude:true
# Challenge 1 - Solution ![:cube]()


```r
qplot(x_test, y_test_obs) +
  geom_line(aes(y = y_test_fit)) +
  theme_bw()
```

---
exclude:true
# Challenge 1 - Solution ![:cube]()


```r
nested_gam_model_test
```

**Answer** Yes non-linearity is justified. The estimated degrees of freedom (**edf**) are &gt;&gt; 1 (we'll get back to this soon).

---
# How GAMs work

We will now take a few minutes to look at what GAMs are doing behind the scenes. Let us first consider a simple model containing one smooth function `\(f\)` of one covariate, `\(x\)`:

`$$y_i = f(x_i) + \epsilon_i$$`

To estimate the smooth function `\(f\)`, we need to represented the above equation in such a way that it becomes a linear model. This can be done by defining basis functions, `\(b_j(x)\)`, of which `\(f\)` is composed:

`$$f(x) = \sum_{j=1}^q b_j(x) \times \beta_j$$`
---
# Example: a polynomial basis

Suppose that `\(f\)` is believed to be a 4th order polynomial, so that the space of polynomials of order 4 and below contains `\(f\)`. A basis for this space would then be:

`$$b_0(x)=1 \ , \quad b_1(x)=x \ , \quad b_2(x)=x^2 \ , \quad b_3(x)=x^3 \ , \quad b_4(x)=x^4$$`

so that `\(f(x)\)` becomes:

`$$f(x) = \beta_0 + x\beta_1 +  x^2\beta_2 + x^3\beta_3 + x^4\beta_4$$`

and the full model now becomes:

`$$y_i = \beta_0 + x_i\beta_1 +  x^2_i\beta_2 + x^3_i\beta_3 + x^4_i\beta_4 + \epsilon_i$$`
---
# Example: a polynomial basis


The basis functions are each multiplied by a real valued parameter, `\(\beta_j\)`, and are then summed to give the &lt;font color="orange"&gt;final curve `\(f(x)\)`&lt;/font&gt;.

.center[
![:scale 85%](images/polynomial_basis_example.png)
]


By varying the `\(\beta_j\)` we can vary the form of `\(f(x)\)` to produce any polynomial function of order 4 or lower.

---
# Example: a cubic spline basis

A cubic spline is a curve constructed from sections of a cubic polynomial joined together so that they are continuous in value. Each section of cubic has different coefficients.

.center[
![](images/cubic_spline.png)
]

---
# Example: a cubic spline basis

Here is a representation of a smooth function using a rank 5 cubic spline basis with knot locations at increments of 0.2:

.center[
![:scale 40%](images/cubic_spline5.jpg)
]

Here, the knots are evenly spaced through the range of observed x values. However, the choice of the degree of model smoothness is controlled by the the number of knots, which was arbitrary.

.comment[Is there a better way to select the knot locations?]

---
# Controlling the degree of smoothing with penalized regression splines

Instead of controlling smoothness by altering the number of knots, we keep that fixed to size a little larger than reasonably necessary, and control the model’s smoothness by adding a “wiggleness” penalty.

So, rather than fitting the model by minimizing (as with least squares regression):

`$$||y - XB||^{2}$$`

it can be fit by minimizing:

`$$||y - XB||^{2} + \lambda \int_0^1[f^{''}(x)]^2dx$$`

As `\(\lambda\)` goes to infinity, the model becomes linear.

---
# Controlling the degree of smoothing with penalized regression splines

If `\(\lambda\)` is too high then the data will be over smoothed, and if it is too low then the data will be under smoothed.

Ideally, it is best to choose `\(\lambda\)` so that the predicted `\(\hat{f}\)` is as close as possible to `\(f\)`. A suitable criterion might be to choose `\(\lambda\)` to minimize:

`$$M = 1/n \times \sum_{i=1}^n (\hat{f_i} - f_i)^2$$`

Since `\(f\)` is unknown, `\(M\)` must be estimated. The recommend methods for this are maximum likelihood (*ML*) or restricted maximum likelihood estimation (*REML*). Generalized cross validation (*GCV*) is another possibility.

---
exclude:true
# The principle behind cross validation

.center[
![:scale 70%](images/smooth_sel.png)
]

1. fits many of the data poorly and does no better with the missing point.



2. fits the underlying signal quite well, smoothing through the noise and the missing datum is reasonably well predicted.



3. fits the noise as well as the signal and the extra variability induced causes it to predict the missing datum rather poorly.



---
exclude:true
# Principle behind cross validation

.center[
![](images/gcv.png)
]


---

class: inverse, center, middle

## 3. GAM with multiple smooth terms

---
# GAM with multiple variables

GAMs make it easy to include both smooth and linear terms, multiple smoothed terms, and smoothed interactions.

For this section, we will use the `ISIT` data again.


```r
head(isit)
isit$Season &lt;- as.factor(isit$Season)
```

We will try to model the response `Sources` using the predictors `Season` and `SampleDepth` simultaneously.

.comment[`Season` needs to be converted into a factor] 

---
# GAM with multiple variables

Let us start with a basic model, with one smoothed term (`SampleDepth`) and one categorical predictor (`Season`, which has 2 levels).



```r
isit$Season &lt;- as.factor(isit$Season)
basic_model &lt;- gam(Sources ~ Season + s(SampleDepth), data = isit, method = "REML")
basic_summary &lt;- summary(basic_model)
```

The `p.table` provides information on the linear terms:


```r
basic_summary$p.table
#             Estimate Std. Error  t value     Pr(&gt;|t|)
# (Intercept) 7.253273  0.3612666 20.07734 1.430234e-72
# Season2     6.156130  0.4825491 12.75752 5.525673e-34
```

The `s.table` provides information on the smooth term:


```r
basic_summary$s.table
#                     edf   Ref.df        F p-value
# s(SampleDepth) 8.706426 8.975172 184.3583       0
```



---
exclude:true
# GAM with multiple variables

GAMs make it easy to include both smooth and linear terms, multiple smoothed terms, and smoothed interactions.

For this section, we will use simulated data generated using `mgcv::gamSim()`.


```r
# ?gamSim
gam_data &lt;-  gamSim(eg = 5)
head(gam_data)
```

We will try to model the response `y` using the predictors `x0` to `x3`.

---
exclude:true
# GAM with multiple variables

Let's start with a basic model, with one smoothed term (x1) and one categorical predictor (x0, which has 4 levels).


```r
basic_model &lt;- gam(y ~ x0 + s(x1), data = gam_data)
basic_summary &lt;- summary(basic_model)
basic_summary$p.table

basic_summary$s.table
```

.comment[The `p.table` provides the significance table for each linear term

The `s.table` provides the significance table for each smoothed term.
]

---

# GAM with multiple variables


```r
plot(basic_model, all.terms = TRUE,page=1)
```

&lt;img src="workshop08-pres-en_files/figure-html/unnamed-chunk-34-1.png" width="864" style="display: block; margin: auto;" /&gt;

--

There is a pronounced difference in bioluminescence between the seasons.

???
Bioluminescence varies non-linearly across the `SampleDepth` gradient, with highest levels of bioluminescence at the surface, followed by a second but smaller peak just above a depth of 1500, with declining levels at deeper depths.

There is also a pronounced difference in bioluminescence between the seasons, with high levels during Season 2, compared to Season 1.

---
# Effective degrees of freedom (EDF)


```r
basic_summary$s.table
#                     edf   Ref.df        F p-value
# s(SampleDepth) 8.706426 8.975172 184.3583       0
```

The `edf` shown in the `s.table` are the **effective degrees of freedom** (EDF) of the the smooth term - essentially, more EDF imply more complex, wiggly splines.

- A value close to 1 tends to be close to a linear term.

- A higher value means that the spline is more wiggly (non-linear).

&gt; In our basic model the EDF of the smooth function `s(SampleDepth)` are ~9, which suggests a highly non-linear curve.

---
# Effective degrees of freedom (EDF)

The **EDF** of a GAM are estimated differently from the degrees of freedom in a linear regression.

In linear regression, the *model* degrees of freedom are equivalent to the number of non-redundant free parameters `\(p\)` in the model (and the *residual* degrees of freedom are given by `\(n-p\)`).

Because the number of free parameters in GAMs is difficult to define, the **EDF** are related to the smoothing parameter `\(\lambda\)`, such that the greater the penalty, the smaller the **EDF**.

---
# Basis dimension and EDF

An upper bound on the **EDF** is determined by the basis dimension `\(k\)` for each smooth function (the **EDF** cannot exceed `\(k-1\)`)

In practice, the exact choice of `\(k\)` is arbitrary, but it should be **large enough** to accommodate a sufficiently complex smooth function.

We will talk about choosing `\(k\)` in section 5.

---
# GAM with multiple linear and smooth terms

We can add a second term, `RelativeDepth`, but specify a linear relationship with `Sources`


```r
two_term_model &lt;- gam(Sources ~ Season + s(SampleDepth) + RelativeDepth, 
                      data = isit, method = "REML")
two_term_summary &lt;- summary(two_term_model)
```

Information on parametric effects (linear terms):


```r
two_term_summary$p.table
#                   Estimate   Std. Error   t value     Pr(&gt;|t|)
# (Intercept)    9.808305503 0.6478741951 15.139213 1.446613e-45
# Season2        6.041930627 0.4767977508 12.671894 1.380010e-33
# RelativeDepth -0.001401908 0.0002968443 -4.722705 2.761048e-06
```

Information on additive effects (non-linear terms):


```r
two_term_summary$s.table
#                     edf  Ref.df        F p-value
# s(SampleDepth) 8.699146 8.97396 132.4801       0
```

???
The regression coefficient which is estimated for this new linear term, `RelativeDepth`, will appear in the `p.table`. Remember, the `p.table` shows information on the parametric effects (linear terms)

In the `s.table`, we will once again find the non-linear smoother, `s(SampleDepth)`, and its wiggleness parameter (`edf`). Remember, the `s.table` shows information on the additive effects (non-linear terms)

---

# GAM with multiple variables

We can add a second term (`RelativeDepth`) to our basic model, but specify a linear relationship with `Sources`.


```r
plot(two_term_model, page = 1, all.terms = TRUE)
```

&lt;img src="workshop08-pres-en_files/figure-html/unnamed-chunk-39-1.png" width="720" style="display: block; margin: auto;" /&gt;

---
# GAM with multiple variables

If we want to know whether the relationship between `Sources` and `RelativeDepth` is
non-linear, we can model `RelativeDepth` as a smooth term instead.


```r
two_smooth_model &lt;- gam(Sources ~ Season + s(SampleDepth) + s(RelativeDepth), 
                        data = isit, method = "REML")
two_smooth_summary &lt;- summary(two_smooth_model)
```

Information on parametric effects (linear terms):


```r
two_smooth_summary$p.table
#             Estimate Std. Error  t value     Pr(&gt;|t|)
# (Intercept) 7.937755  0.3452945 22.98836 1.888513e-89
# Season2     4.963951  0.4782280 10.37988 1.029016e-23
```

Information on additive effects (non-linear terms):


```r
two_smooth_summary$s.table
#                       edf   Ref.df         F p-value
# s(SampleDepth)   8.752103 8.973459 150.37263       0
# s(RelativeDepth) 8.044197 8.749580  19.97476       0
```

---
# GAM with multiple variables


```r
plot(two_smooth_model, page = 1, all.terms = TRUE)
```

&lt;img src="workshop08-pres-en_files/figure-html/unnamed-chunk-43-1.png" width="720" style="display: block; margin: auto;" /&gt;

---
# GAM with multiple variables

As before, we can compare our models with AIC to test whether the smoothed term improves our model's performance.


```r
AIC(basic_model, two_term_model, two_smooth_model)
#                        df      AIC
# basic_model      11.83374 5208.713
# two_term_model   12.82932 5188.780
# two_smooth_model 20.46960 5056.841
```

.alert[The best fit model is the model with both smooth terms for `SampleDepth` and `RelativeDepth`]
---
exclude:true
# GAM with multiple variables

We can add a second term, `x2`, but specify a linear relationship with `y`


```r
two_term_model &lt;- gam(y ~ x0 + s(x1) + x2, data = gam_data)
two_term_summary &lt;- summary(two_term_model)
two_term_summary$p.table

two_term_summary$s.table
```

---
exclude:true
# GAM with multiple variables

We can add a second term, `x2`, but specify a linear relationship with `y`


```r
plot(two_term_model)
```


---
exclude:true
# GAM with multiple variables

We can also explore whether the relationship between `y` and `x2` is non-linear


```r
two_smooth_model &lt;- gam(y ~ x0 + s(x1) + s(x2), data = gam_data)
two_smooth_summary &lt;- summary(two_smooth_model)
two_smooth_summary$p.table

two_smooth_summary$s.table
```

---
exclude:true
# GAM with multiple variables

We can also explore whether the relationship between `y` and `x2` is non-linear


```r
plot(two_smooth_model, page = 1)
```

---
exclude:true
# GAM with multiple variables

As before, we can perform an ANOVA to test if the smoothed term is necessary


```r
anova(basic_model, two_term_model, two_smooth_model,test='Chisq')
```

.alert[The best fit model is the model with both smooth terms for `x1` and `x2`]
---
# Challenge 2 ![:cube]()

&lt;br&gt;

For our second challenge, we will be building onto our model by adding variables which we think might be ecologically significant predictors to explain bioluminescence. 

1. Create two new models: Add `Latitude` to `two_smooth_model`, first as a linear term, then as a smooth term.
2. Is `Latitude` an important term to include? Does `Latitude` have a linear or additive effect? 

Use plots, coefficient tables, and the `AIC()` function to help you answer this question.

---
exclude:true
# Challenge 2 ![:cube]()

&lt;br&gt;

1. Create 2 new models, with `x3` as a linear and smoothed term.
2. Determine if `x3` is an important term to include using plots, coefficient tables and the anova function.
---
# Challenge 2 - Solution ![:cube]()

&lt;br&gt;

```r
# Add Latitude as a linear term
three_term_model &lt;- gam(Sources ~ 
                          Season + s(SampleDepth) + s(RelativeDepth) + 
                          Latitude, 
                        data = isit, method = "REML")
three_term_summary &lt;- summary(three_term_model)

# Add Latitude as a smooth term
three_smooth_model &lt;- gam(Sources ~ 
                            Season + s(SampleDepth) + s(RelativeDepth) + 
                            s(Latitude),
                          data = isit, method = "REML")
three_smooth_summary &lt;- summary(three_smooth_model)
```

---
exclude:true
# Challenge 2 - Solution ![:cube]()

&lt;br&gt;

```r
three_term_model &lt;- gam(y ~ x0 + s(x1) + s(x2) + x3, data = gam_data)
three_smooth_model &lt;- gam(y ~ x0 + s(x1) + s(x2) + s(x3), data = gam_data)
three_smooth_summary &lt;- summary(three_smooth_model)
```

---
# Challenge 2 - Solution ![:cube]()

__2.__ Is `Latitude` an important term to include? Does `Latitude` have a linear or additive effect?

Let us begin by plotting the the 4 effects that are now included in each model.

---
# Challenge 2 - Solution ![:cube]()


```r
plot(three_term_model, page = 1, all.terms = TRUE)
```

&lt;img src="workshop08-pres-en_files/figure-html/unnamed-chunk-53-1.png" width="576" style="display: block; margin: auto;" /&gt;

---
# Challenge 2 - Solution ![:cube]()


```r
plot(three_smooth_model, page = 1, all.terms = TRUE)
```

&lt;img src="workshop08-pres-en_files/figure-html/unnamed-chunk-55-1.png" width="576" style="display: block; margin: auto;" /&gt;

---
# Challenge 2 - Solution ![:cube]()

We should also look at our coefficient tables. What can the EDF tell us about the _wiggliness_ of our predictors' effects?


```r
three_smooth_summary$s.table
#                       edf   Ref.df         F p-value
# s(SampleDepth)   8.766891 8.975682 68.950905       0
# s(RelativeDepth) 8.007411 8.730625 17.639321       0
# s(Latitude)      7.431116 8.296838  8.954349       0
```

--

The EDF are all quite high for all of our smooth variables, including `Latitude`. 

This tells us that `Latitude` is quite _wiggly_, and probably should not be included as a linear term.

---
# Challenge 2 - Solution ![:cube]()

Before deciding which model is "best", we should test whether `Latitude` is best included as a linear or as a smooth term, using `AIC()`:


```r
AIC(three_smooth_model, three_term_model)
#                          df      AIC
# three_smooth_model 28.20032 4990.546
# three_term_model   21.47683 5051.415
```

--
Our model including Latitude as a _smooth_ term has a lower AIC score, meaning it performs better than our model including Latitude as a _linear_ term. 

--
But, does adding `Latitude` as a smooth predictor actually improve on our last "best" model (`two_smooth_model`)?

---
# Challenge 2 - Solution ![:cube]()


```r
AIC(two_smooth_model, three_smooth_model)
#                          df      AIC
# two_smooth_model   20.46960 5056.841
# three_smooth_model 28.20032 4990.546
```

Our `three_smooth_model` has a lower AIC score than our previous best model (`two_smooth_model`), which did not include `Latitude`. 

--

This implies that `Latitude` is indeed an informative non-linear predictor of bioluminescence.


---
exclude:true
# Challenge 2 ![:cube]()

&lt;br&gt;

1. Create 2 new models, with `x3` as a linear and smoothed term.
2. Determine if `x3` is an important term to include using plots, coefficient tables and the AIC function.

---
exclude:true
# Challenge 2 - Solution ![:cube]()

&lt;br&gt;

```r
three_term_model &lt;- gam(y ~ x0 + s(x1) + s(x2) + x3, data = gam_data)
three_smooth_model &lt;- gam(y~x0 + s(x1) + s(x2) + s(x3), data = gam_data)
three_smooth_summary &lt;- summary(three_smooth_model)
```

---
exclude:true
# Challenge 2 - Solution ![:cube]()


```r
plot(three_smooth_model, page = 1)
```

&lt;img src="workshop08-pres-en_files/figure-html/unnamed-chunk-61-1.png" width="576" style="display: block; margin: auto;" /&gt;

---
exclude:true
# Challenge 2 - Solution ![:cube]()


```r
three_smooth_summary$s.table

# edf = 1 therefore term is linear.

AIC(two_smooth_model, three_term_model, test = "Chisq")

# term x3 is not significant, it should be dropped!
```


---

class: inverse, center, middle

## 4. Interactions

---
# GAM with interaction terms

There are 2 ways to include interactions between variables:

- for 2 smoothed variables : `s(x1, x2)`
- for one smoothed variable and one linear variable (either factor or continuous): use the `by` argument `s(x1, by = x2)`
  - When `x2` is a factor, you have a smooth term that vary between different levels of `x2`
  - When `x2` is continuous, the linear effect of `x2` varies smoothly with `x1`
  - When `x2` is a factor, the factor needs to be added as a main effect in the model

---
# GAM with interaction terms

We will examine interaction effect using our categorical variable `Season` and ask whether the non-linear smoother `s(SampleDepth)` varies across different levels of `Season`.


```r
factor_interact &lt;- gam(Sources ~ Season + 
                         s(SampleDepth, by=Season) + 
                         s(RelativeDepth), 
                       data = isit, method = "REML")

summary(factor_interact)$s.table
#                             edf   Ref.df          F p-value
# s(SampleDepth):Season1 6.839386 7.552045  95.119422       0
# s(SampleDepth):Season2 8.744574 8.966290 154.474325       0
# s(RelativeDepth)       6.987223 8.055898   6.821074       0
```

---
exclude:true
# GAM with interaction terms

We will examine interaction effect using our categorical variable `x0` and ask whether the non-linear smoother `s(x2)` varies across different levels of `x0`.


```r
factor_interact &lt;- gam(y ~ x0 + s(x1) + s(x2, by = x0), data = gam_data)

summary(factor_interact)$s.table
```

---
# GAM with interaction terms


```r
plot(factor_interact, page = 1)
```

&lt;img src="workshop08-pres-en_files/figure-html/unnamed-chunk-65-1.png" width="720" style="display: block; margin: auto;" /&gt;

---
exclude:true
# GAM with interaction terms

We can also visualise our model in 3D using `vis.gam`, where `theta` is the degree rotation on the x-y plane


```r
vis.gam(factor_interact, view = c("x2","x0"), theta = 40, n.grid = 500, border = NA)
```

---
# GAM with interaction terms

Let's perform a model comparison using AIC to determine if the interaction term is necessary


```r
AIC(two_smooth_model, factor_interact)
#                        df      AIC
# two_smooth_model 20.46960 5056.841
# factor_interact  26.99693 4878.631
```

From the plots, we saw that the shape of the smooth terms were comparable among the 2 levels of `Season`. The AIC confirms this as well.

---
# GAM with interaction terms

Finally we'll look at the interactions between 2 smoothed terms, `SampleDepth` and `RelativeDepth`.


```r
smooth_interact &lt;- gam(Sources ~ Season + s(SampleDepth, RelativeDepth), 
                       data = isit, method = "REML")
summary(smooth_interact)$s.table
#                                   edf Ref.df        F p-value
# s(SampleDepth,RelativeDepth) 27.12521  28.77 93.91722       0
```

--

&lt;img src="workshop08-pres-en_files/figure-html/unnamed-chunk-69-1.png" width="504" style="display: block; margin: auto;" /&gt;

---
exclude:true
# GAM with interaction terms

Finally we'll look at the interactions between 2 smoothed terms, `x1` and `x2`.


```r
smooth_interact &lt;- gam(y~x0 + s(x1, x2), data = gam_data)
summary(smooth_interact)$s.table
```

&lt;img src="workshop08-pres-en_files/figure-html/unnamed-chunk-71-1.png" width="504" style="display: block; margin: auto;" /&gt;


---

# GAM with interaction terms



```r
vis.gam(smooth_interact, view = c("SampleDepth", "RelativeDepth"), 
        cond = list(Season = 1), theta=40, n.grid = 50, color = "cm")
```

&lt;img src="workshop08-pres-en_files/figure-html/unnamed-chunk-72-1.png" width="576" style="display: block; margin: auto;" /&gt;


---
# GAM with interaction terms


```r
AIC(two_smooth_model, smooth_interact)
#                        df      AIC
# two_smooth_model 20.46960 5056.841
# smooth_interact  30.33625 4943.890
```

The model with the interaction between `s(SampleDepth)` and `s(RelativeDepth)` has a lower AIC and the plots nicely illustrate the non-linear interaction, where `Sources` is lower at high values of `SampleDepth` but increases with `RelativeDepth`.



---
class: inverse, center, middle

## 5. Generalization of the additive model

---

# Generalization of the additive model

The basic additive model can be extended in the following way:

1. Using other **distributions** for the response variable with the `family` argument (just as in a GLM),
2. Using different kinds of **basis functions**,
3. Using different kinds of **random effects** to fit mixed effect models.

We will now go over these aspects.

---

# Generalized additive models

We have so far worked with simple (Gaussian) additive models, the non-linear equivalent to a linear model.

--

But what can we do if:
- the observations of the response variable do **not follow a normal distribution**?
- the **variance is not constant**? (heteroscedasticity)

--

.comment[These cases are very common!]

Just like generalized linear models (GLM), we can formulate **generalized** additive models to deal with these issues.

---

# Generalized additive models


Lets recall the interaction model for the bioluminescence data:


```r
smooth_interact &lt;- gam(Sources ~ Season + s(SampleDepth, RelativeDepth), 
                       data = isit, method = "REML")

summary(smooth_interact)$p.table
#             Estimate Std. Error   t value     Pr(&gt;|t|)
# (Intercept) 8.077356  0.4235432 19.070912 1.475953e-66
# Season2     4.720806  0.6559436  7.196969 1.480113e-12

summary(smooth_interact)$s.table
#                                   edf Ref.df        F p-value
# s(SampleDepth,RelativeDepth) 27.12521  28.77 93.91722       0
```

---

# GAM model checking

As for a GLM, it is essential to check whether we correctly specifified the model, especially the *distribution* for the response.

**We have to verify:**

1. The choice of basis dimension `k`.
2. The residuals plots (just as for a GLM).


--

&lt;br&gt;
Useful functions included in `mgcv`:

- `k.check()` performs a basis dimension check.
- `gam.check()` produces residual plots (and also calls `k.check()`).

---

# GAM model checking

##### First step:

Have we chosen `k` large enough? 

.comment[The default for smooth interactions is `k = 30`]


```r
k.check(smooth_interact)
#                              k'      edf   k-index p-value
# s(SampleDepth,RelativeDepth) 29 27.12521 0.9448883   0.055
```

--

The **EDF are very close to** `k`, this could be problematic.

---

# GAM model checking

Let's refit the model with a larger `k`:


```r
smooth_interact_k60 &lt;- gam(Sources ~ Season + s(SampleDepth, RelativeDepth, k = 60), 
                           data = isit, method = "REML")
summary(smooth_interact_k60)$p.table
#             Estimate Std. Error   t value     Pr(&gt;|t|)
# (Intercept) 8.129512  0.4177659 19.459491 1.911267e-68
# Season2     4.629964  0.6512205  7.109672 2.741865e-12
summary(smooth_interact_k60)$s.table
#                                   edf   Ref.df        F p-value
# s(SampleDepth,RelativeDepth) 46.03868 54.21371 55.19817       0
```

--

Is `k` large enough this time?


```r
k.check(smooth_interact_k60)
#                              k'      edf  k-index p-value
# s(SampleDepth,RelativeDepth) 59 46.03868 1.048626  0.8925
```

--

Looks better, let's replace the old model:


```r
smooth_interact &lt;- smooth_interact_k60
```

---

# GAM model checking

##### Second Step:

Let's look at the residual plots, using `gam.check()`:


```r
par(mfrow = c(2,2)) # Show all 4 plots side by side
gam.check(smooth_interact)
```

.comment[In addition to the plots, `gam.check()` also provides the output of `k.check()`.]

---

# GAM model checking

&lt;img src="workshop08-pres-en_files/figure-html/unnamed-chunk-80-1.png" width="576" style="display: block; margin: auto;" /&gt;

--
&lt;br&gt;

.alert[Pronounced heteroscedasticity and patterns in the residuals]


???

- These plots are a little different than those produced by `plot` for a linear model (e.g. no leverage plot)
- Participants should already be familiar with residual plots (they are explaned more in detail in workshops 4 and 6)
- Obvious problem: heteroscedasticity, visible in residuals vs. linear predictor.
- Another problem: a few strong outliers (visible in QQ plot, response vs. fitted)

---

# Other distributions


For our interaction model, we need a probability distribution that allows the **variance to increase with the mean**.

--

One family of distributions that has this property and that works well in a GAM is the **Tweedie** family.

A common link function for *Tweedie* distributions is the `\(log\)`.

--

&lt;br&gt;

As in a GLM, we can use the `family = ` argument in `gam()` to fit models with other distributions (including distributions such as `binomial`, `poisson`, `gamma` etc.).

To get an overview of families available in `mgcv`:

```r
?family.mgcv
```




---

# Challenge 3 ![:cube]()

1. Fit a new model `smooth_interact_tw` with the same formula as the `smooth_interact` model but with a distribution from the *Tweedie* family (instead of the normal distribution) and `log` link function. You can do so by using `family = tw(link = "log")` inside `gam()`.
2. Check the choice of `k` and the residual plots for the new model.
3. Compare `smooth_interact_tw` with `smooth_interact`. Which one would you choose?

--

&lt;br&gt;

.comment[Hint:]


```r
# Normal distribution
smooth_interact &lt;- gam(Sources ~ Season + s(SampleDepth, RelativeDepth, k = 60), 
                          data = isit, method = "REML")
# Tweedie family with log link
smooth_interact_tw &lt;- gam(Sources ~ Season + s(SampleDepth, RelativeDepth, k = 60), 
                          family = tw(link = "log"),
                          data = isit, method = "REML")
```


---

# Challenge 3 - Solution ![:cube]()

Fit the model:


```r
smooth_interact_tw &lt;- gam(Sources ~ Season + s(SampleDepth, RelativeDepth, k = 60), 
                          family = tw(link = "log"),
                          data = isit, method = "REML")
summary(smooth_interact_tw)$p.table
#              Estimate Std. Error  t value      Pr(&gt;|t|)
# (Intercept) 1.3126641 0.03400390 38.60333 8.446478e-180
# Season2     0.5350529 0.04837342 11.06089  1.961733e-26
summary(smooth_interact_tw)$s.table
#                                   edf   Ref.df        F p-value
# s(SampleDepth,RelativeDepth) 43.23949 51.57139 116.9236       0
```

--

Check the basis dimension:


```r
k.check(smooth_interact_tw)
#                              k'      edf  k-index p-value
# s(SampleDepth,RelativeDepth) 59 43.23949 1.015062    0.81
```

---

# Challenge 3 - Solution ![:cube]()

Residual plots:


```r
par(mfrow=c(2,2))
gam.check(smooth_interact_tw)
```


&lt;img src="workshop08-pres-en_files/figure-html/unnamed-chunk-86-1.png" width="576" style="display: block; margin: auto;" /&gt;


???
- The residuals look much better, but it is clear that something is missing from the model. This could be a spatial affect (longtitude and latitude), or a random effect (e.g. based on `Station`).

---

# Challenge 3 - Solution ![:cube]()

Compare the models:


```r
AIC(smooth_interact, smooth_interact_tw)
#                          df      AIC
# smooth_interact    49.47221 4900.567
# smooth_interact_tw 47.86913 3497.733
```

.comment[AIC allows us to compare models that are based on different distributions!]

--

Using a *Tweedie* instead of a *normal* distribution greatly improves our model!


---
class: inverse, center, middle

## 6. Changing basis


---
# Other smooth functions

To model a non-linear smooth variable or surface, smooth functions can be built in different ways:

`s()` ![:faic](arrow-right) for modeling a 1-dimensional smooth or for modeling interactions among variables measured using the same unit and the same scale

`te()` ![:faic](arrow-right)  for modeling 2- or n-dimensional interaction surfaces of variables that are not on the same scale. Includes main effects.

`ti()` ![:faic](arrow-right) for modeling 2- or n-dimensional interaction surfaces that do not include the main effects.

---
# Parameters of smooth functions

The smooth functions have several parameters that can be set to change their behavior. The most often-used parameters are :

`k` ![:faic](arrow-right) basis dimension
  - determines the upper bound of the number of base functions used to build the curve.
  - constrains the wigglyness of a smooth.
  - the default `\(k\)` is 10 for `s()`, and 5 for `te()` and `ti()`.
  - k should be &lt; the number of unique data points.
  - the comlexity (i.e. non-linearity) of a smooth function in a fitted model is reflected by its effective degrees of freedom (**EDF**).


---
# Parameters of smooth functions

The smooth functions have several parameters that can be set to change their behavior. The most often-used parameters are :

`d` ![:faic](arrow-right) specifies that predictors in the interaction are on the same scale or dimension (only used in `te()` and `ti()`).
  - For example, in `te(Time, width, height, d=c(1,2))`, indicates that `width` and `height` are one the same scale, but not `Time`.

`bs` ![:faic](arrow-right) specifies the type of underlying base functions.
  - the default for `s()` is `tp` (thin plate regression spline) and for `te()` and `ti()` is `cr` (cubic regression spline).

&lt;!--last 2 slides were a copy-paste from this websites: http://www.sfs.uni-tuebingen.de/~jvanrij/Tutorial/GAMM.html. However, as I'm not a specialist of GAM, I let them asis, but we should modify them or at least site the website--&gt;

---
# Example smooth for cyclical data

Cyclical data is a good example where changing basis is useful: you want the predictor to match at the ends.

Let's use a time series of climate data, with monthly measurements, and see if there's a temporal trend in yearly temperature.


```r
data(nottem) # Nottingham temperature time series
n_years &lt;- length(nottem)/12
nottem_month &lt;- rep(1:12, times = n_years)
nottem_year &lt;- rep(1920:(1920 + n_years - 1), each = 12)
qplot(nottem_month, nottem, colour = factor(nottem_year), geom = "line") +
  theme_bw()
```

---
# Example smooth for cyclical data

&lt;img src="workshop08-pres-en_files/figure-html/unnamed-chunk-89-1.png" width="864" style="display: block; margin: auto;" /&gt;

---
# Example smooth for cyclical data

We can model both the cyclic change of temperature across months and the non-linear trend through years, using a cyclical cubic spline, or `cc`, for the month variable and a regular smooth for the year variable.


```r
year_gam &lt;- gam(nottem ~ s(nottem_year) + s(nottem_month, bs = "cc"), method = "REML")
summary(year_gam)$s.table
#                      edf   Ref.df          F    p-value
# s(nottem_year)  1.621375 2.011475   2.850888 0.06141004
# s(nottem_month) 6.855132 8.000000 393.119285 0.00000000
```

---
# Example smooth for cyclical data


```r
plot(year_gam, page = 1, scale = 0)
```

&lt;img src="workshop08-pres-en_files/figure-html/unnamed-chunk-91-1.png" width="576" style="display: block; margin: auto;" /&gt;

There is about 1-1.5 degree rise in temperature over the period, but within a given year there is about 20 degrees variation in temperature, on average. The actual data vary around these values and that is the unexplained variance.



---
class: inverse, center, middle

# 7. Quick intro to GAMM

---
# Dealing with non-independence

When observations are not independent, GAMs can be used to either incorporate:

- a serial correlation structure to model residual autocorrelation (autoregressive AR, moving average MA, or a combination of the two ARMA),
- random effects that model independence among observations from the same site.

---
# Model with correlated errors

Let's have a look at a model with temporal autocorrelation in the residuals. We will revisit the Nottingham temperature model and test for correlated errors using the (partial) autocorrelation function.


```r
par(mfrow = c(1,2))
acf(resid(year_gam), lag.max = 36, main = "ACF")
pacf(resid(year_gam), lag.max = 36, main = "pACF")
```

---
# Model with correlated errors

&lt;img src="workshop08-pres-en_files/figure-html/unnamed-chunk-93-1.png" width="648" style="display: block; margin: auto;" /&gt;

.comment[ACF evaluates the cross correlation and pACF the partial correlation of a time series with itself at different time lags]

ACF and pACF are used to identify after how many time steps observations start to be independent.

--

The ACF plot of our model residuals suggests a significant lag of 1, and perhaps a lag of 2. Therefore, a low-order AR model is likely needed.

---
# Model with correlated errors

We can add AR structures to the model: 

- **AR(1)**: correlation at 1 time step, or 
- **AR(2)** correlation at 2 time steps.

--


```r
year_gam &lt;- gamm(nottem ~ s(nottem_year) + s(nottem_month, bs = "cc"))
year_gam_AR1 &lt;- gamm(nottem ~ s(nottem_year) + s(nottem_month, bs = "cc"),
                     correlation = corARMA(form = ~ 1|nottem_year, p = 1),
                   data = data.frame(nottem, nottem_year, nottem_month))
year_gam_AR2 &lt;- gamm(nottem ~ s(nottem_year) + s(nottem_month, bs = "cc"),
                     correlation = corARMA(form = ~ 1|nottem_year, p = 2),
                   data = data.frame(nottem, nottem_year, nottem_month))
```


---

# Model with correlated errors


```r
AIC(year_gam$lme, year_gam_AR1$lme, year_gam_AR2$lme)
#                  df      AIC
# year_gam$lme      5 1109.908
# year_gam_AR1$lme  6 1101.218
# year_gam_AR2$lme  7 1101.598
```

AR(1) provides a better fit over the naive model. But there is little improvement with AR(2).


---
# Mixed modelling

As we saw in the section changing basis, `bs` specifies the type of underlying base function. For random intercepts and linear random slopes we use `bs = "re"`, but for random smooths we use `bs = "fs"`.

--

**3 different types of random effects** in GAMMs (`fac` ![:faic](arrow-right) factor coding for the random effect; `x0` ![:faic](arrow-right) continuous fixed effect):

- **random intercepts** adjust the height of other model terms with a constant value: `s(fac, bs = "re")`
- **random slopes** adjust the slope of the trend of a numeric predictor: `s(fac, x0, bs = "re")`
- **random smooths** adjust the trend of a numeric predictor in a nonlinear way: `s(x0, fac, bs = "fs", m = 1)`, where the argument m=1 sets a heavier penalty for the smooth moving away from 0, causing shrinkage to the mean.

---
# GAMM with a random intercept

As before, we will use the `gamSim()` function to generate a dataset, here with a random effect, then run a model with a random intercept using `fac` as the random factor.


```r
gam_data2 &lt;- gamSim(eg = 6)
# 4 term additive + random effectGu &amp; Wahba 4 term additive model
str(gam_data2)
# 'data.frame':	400 obs. of  11 variables:
#  $ y  : num  6.61 12.11 22.21 19.28 14.42 ...
#  $ x0 : num  0.322 0.523 0.318 0.499 0.307 ...
#  $ x1 : num  0.748 0.244 0.979 0.834 0.825 ...
#  $ x2 : num  0.8672 0.5927 0.6929 0.0324 0.3031 ...
#  $ x3 : num  0.8037 0.0971 0.411 0.7687 0.3778 ...
#  $ f  : num  9.38 12.78 20.75 19.54 17.42 ...
#  $ f0 : num  1.69 1.99 1.68 2 1.64 ...
#  $ f1 : num  4.46 1.63 7.08 5.3 5.2 ...
#  $ f2 : num  0.229 3.157 2.99 0.245 7.57 ...
#  $ f3 : num  0 0 0 0 0 0 0 0 0 0 ...
#  $ fac: Factor w/ 4 levels "1","2","3","4": 1 2 3 4 1 2 3 4 1 2 ...
```


---
# GAMM with a random intercept


```r
gamm_intercept &lt;- gam(y ~ s(x0) + s(fac, bs = "re"), data = gam_data2, method = "REML")
summary(gamm_intercept)$s.table
#             edf   Ref.df          F   p-value
# s(x0)  1.914776 2.385637   1.368419 0.2727416
# s(fac) 2.973595 3.000000 112.703655 0.0000000
plot(gamm_intercept, select = 2)
```

&lt;img src="workshop08-pres-en_files/figure-html/unnamed-chunk-97-1.png" width="396" style="display: block; margin: auto;" /&gt;

---
# GAMM with a random intercept

We can plot the summed effects for the `x0` without random effects, and then plot the predictions of all 4 levels of the random `fac` effect:


```r
par(mfrow = c(1,2), cex = 1.1)

plot_smooth(gamm_intercept, view = "x0", rm.ranef = T,
            main = "intercept + s(x1)")

plot_smooth(gamm_intercept, view = "x0", cond = list(fac="1"),
            main = "... + s(fac)", col = 'orange', ylim = c(8,21))

plot_smooth(gamm_intercept, view = "x0", cond = list(fac = "2"), add = T, col = 'red')

plot_smooth(gamm_intercept, view="x0", cond = list(fac = "3"), add = T, col = 'purple')

plot_smooth(gamm_intercept, view="x0", cond = list(fac = "4"), add = T, col = 'turquoise')
```

---
# GAMM with a random intercept

&lt;br&gt;

&lt;img src="workshop08-pres-en_files/figure-html/unnamed-chunk-99-1.png" width="864" style="display: block; margin: auto;" /&gt;

.pull-right[
&amp;nbsp; &lt;font color="orange"&gt;fac1&lt;/font&gt; &amp;nbsp; &lt;font color="red"&gt;fac2&lt;/font&gt; &amp;nbsp; &lt;font color="purple"&gt;fac3&lt;/font&gt; &amp;nbsp; &lt;font color="turquoise"&gt;fac4&lt;/font&gt;
]


---
# GAMM with a random slope


```r
gamm_slope &lt;- gam(y ~ s(x0) + s(x0, fac, bs = "re"), data = gam_data2, method = "REML")

summary(gamm_slope)$s.table
#                edf   Ref.df         F   p-value
# s(x0)     2.221392 2.760953  1.640112 0.1576433
# s(x0,fac) 2.953123 3.000000 60.148127 0.0000000
```


---
# GAMM with a random slope


```r
par(mfrow = c(1,2), cex = 1.1)

plot_smooth(gamm_slope, view = "x0", rm.ranef = TRUE, main = "intercept + s(x0)")

plot_smooth(gamm_slope, view = "x0", cond = list(fac = "1"),
            main = "... + s(fac)", col = 'orange', ylim = c(7,22))

plot_smooth(gamm_slope, view = "x0", cond = list(fac = "2"), add = T, col = 'red')

plot_smooth(gamm_slope, view = "x0", cond = list(fac = "3"), add = T, col = 'purple')

plot_smooth(gamm_slope, view = "x0", cond = list(fac = "4"), add = T, col = 'turquoise')
```

---
# GAMM with a random slope

&lt;br&gt;

&lt;img src="workshop08-pres-en_files/figure-html/unnamed-chunk-102-1.png" width="864" style="display: block; margin: auto;" /&gt;

---
# GAMM with a random intercept and slope


```r
gamm_int_slope &lt;- gam(y ~ s(x0) + s(fac, bs = "re") + s(fac, x0, bs = "re"),
                      data = gam_data2, method = "REML")

summary(gamm_int_slope)$s.table
#                   edf   Ref.df            F   p-value
# s(x0)     1.914770364 2.385624 1.368114e+00 0.2728299
# s(fac)    2.973543694 3.000000 1.129192e+02 0.0000000
# s(fac,x0) 0.002197389 3.000000 5.352643e-04 0.5886516
```

---
# GAMM with a random intercept and slope


```r
par(mfrow = c(1,2), cex = 1.1)

plot_smooth(gamm_int_slope, view = "x0", rm.ranef = T, main = "intercept + s(x0)")

plot_smooth(gamm_int_slope, view = "x0", cond = list(fac = "1"),
            main="... + s(fac) + s(fac, x0)", col = 'orange', ylim = c(7,22))

plot_smooth(gamm_int_slope, view = "x0", cond = list(fac = "2"), add = T, col='red')

plot_smooth(gamm_int_slope, view = "x0", cond = list(fac = "3"), add = T, col = 'purple')

plot_smooth(gamm_int_slope, view = "x0", cond = list(fac = "4"), add = T, col = 'turquoise')
```

---
# GAMM with a random intercept and slope

&lt;br&gt;

&lt;img src="workshop08-pres-en_files/figure-html/unnamed-chunk-105-1.png" width="864" style="display: block; margin: auto;" /&gt;

---
# GAMM with a random intercept and slope

Note that the random slope is static in this case:


```r
plot(gamm_int_slope, select = 3)
```

&lt;img src="workshop08-pres-en_files/figure-html/unnamed-chunk-106-1.png" width="432" style="display: block; margin: auto;" /&gt;

---
# GAMM with a random smooth


```r
gamm_smooth &lt;- gam(y ~ s(x0) + s(x0, fac, bs = "fs", m = 1), 
                   data = gam_data2, method = "REML")

summary(gamm_smooth)$s.table
#                edf    Ref.df        F   p-value
# s(x0)     1.914528  2.385156 1.366971 0.2731697
# s(x0,fac) 2.985806 35.000000 9.660855 0.0000000
```

---
# GAMM with a random smooth

Here, if the random slope varied along `x0`, we would see different curves for each level:


```r
plot(gamm_smooth, select = 1)
```

&lt;img src="workshop08-pres-en_files/figure-html/unnamed-chunk-108-1.png" width="396" style="display: block; margin: auto;" /&gt;

---
# GAMM with a random smooth


```r
par(mfrow = c(1,2), cex = 1.1)

plot_smooth(gamm_smooth, view = "x0", rm.ranef = T, main = "intercept + s(x0)")

plot_smooth(gamm_smooth, view = "x0", cond = list(fac = "1"),
            main="... + s(x0, fac)", col = 'orange', ylim = c(7,22))

plot_smooth(gamm_smooth, view = "x0", cond = list(fac = "2"), add = T, col='red')

plot_smooth(gamm_smooth, view = "x0", cond = list(fac = "3"), add = T, col = 'purple')

plot_smooth(gamm_smooth, view = "x0", cond = list(fac = "4"), add = T, col = 'turquoise')
```

---
# GAMM with a random smooth

&lt;br&gt;

&lt;img src="workshop08-pres-en_files/figure-html/unnamed-chunk-110-1.png" width="864" style="display: block; margin: auto;" /&gt;

.comment[Here, if the random slope varied along `x0`, we would see different curves for each level.]

---
# GAMM

All of the mixed models from this section can be compared using `AIC()` to determine the best fit model


```r
AIC(gamm_intercept, gamm_slope, gamm_int_slope, gamm_smooth)
#                      df      AIC
# gamm_intercept 7.385361 2228.375
# gamm_slope     7.760149 2324.528
# gamm_int_slope 7.389632 2228.380
# gamm_smooth    7.409065 2228.401
```


---
# Resources

This workshop was a brief introduction to basic concepts, and popular packages to help you estimate, evaluate, and visualise GAMs in R, but there is so much more to explore!
&lt;br&gt;&lt;br&gt;
* The book [Generalized Additive Models: An Introduction with R](https://www.routledge.com/Generalized-Additive-Models-An-Introduction-with-R-Second-Edition/Wood/p/book/9781498728331) by Simon Wood (the author of the `mgcv` package). 
* Simon Wood's website, [maths.ed.ac.uk/~swood34/](https://www.maths.ed.ac.uk/~swood34/).
* Gavin Simpson's blog, [From the bottom of the heap](https://fromthebottomoftheheap.net/). 
* Gavin Simpson's package [`gratia`](https://cran.r-project.org/web/packages/gratia/index.html) for GAM visualisation in `ggplot2`.
* [Generalized Additive Models: An Introduction with R](https://noamross.github.io/gams-in-r-course/) course by Noam Ross.
* [Overview GAMM analysis of time series data](https://jacolienvanrij.com/Tutorials/GAMM.html) tutorial by Jacolien van Rij.
* [Hierarchical generalized additive models in ecology: an introduction with mgcv](https://peerj.com/articles/6876/) by Pedersen et al. (2019).

Finally, the help pages, available through `?gam` in R, are an excellent resource.


---
class: inverse, center, bottom

# Thank you for attending this workshop!

![:scale 50%](images/qcbs_logo.png)

---
class: inverse, center, middle

# Additional example with other distributions


---
# GAM using other distributions

Let's now take a look on how to use GAMs when the response variable does not follow a normal distributions and is either count or proportion data (e.g., Gamma, binomial, Poisson, negative binomial).

We will use an example dataset where a binomial distribution is needed; the response variable represents the number of successes vs failures over the course of an experiment.


```r
gam_data3 &lt;- read.csv("data/other_dist.csv")
str(gam_data3)
# 'data.frame':	514 obs. of  4 variables:
#  $ prop : num  1 1 1 1 0 1 1 1 1 1 ...
#  $ total: int  4 20 20 18 18 18 20 20 20 20 ...
#  $ x1   : int  550 650 750 850 950 650 750 850 950 550 ...
#  $ fac  : chr  "f1" "f1" "f1" "f1" ...
```

&lt;!-- should change the name of the variables in the csv files, to make them meaningful--&gt;

---
# GAM using other distributions


```r
plot(range(gam_data3$x1), c(0,1), type = "n",
     main = "Probability of successes over time",
     ylab = "Probability", xlab = "x1 (time)")
abline(h = 0.5)

avg &lt;- aggregate(prop ~ x1, data = gam_data3, mean)
lines(avg$x1, avg$prop, col = "orange", lwd = 2)
```

&lt;img src="workshop08-pres-en_files/figure-html/unnamed-chunk-113-1.png" width="360" style="display: block; margin: auto;" /&gt;

---
# GAM using other distributions

We will test if this trend is linear or not using a logistic GAM (we use a binomial family distribution given that our response is proportion data).


```r
prop_model &lt;- gam(prop ~ s(x1), data = gam_data3, weights = total, 
                  family = "binomial", method = "REML")
prop_summary &lt;- summary(prop_model)
```

&lt;!--Warning messages:
1: In eval(family$initialize) : non-integer #successes in a binomial glm!??--&gt;

--

.comment[What does the intercept represent in this model?]


```r
prop_summary$p.table
#             Estimate Std. Error  z value Pr(&gt;|z|)
# (Intercept) 1.174024 0.02709868 43.32402        0
```

--

.comment[What does the smooth term indicate?]


```r
prop_summary$s.table
#            edf   Ref.df   Chi.sq p-value
# s(x1) 4.750198 5.791279 799.8463       0
```



---
# GAM using other distributions

.comment[What does the intercept represent in this model?]


```
#             Estimate Std. Error  z value Pr(&gt;|z|)
# (Intercept) 1.174024 0.02709868 43.32402        0
```

Recall that the model uses the count data to calculate the logit, which is the log odds ratio between successes and failures:
.small[
- If successes = failures, the ratio = 1 and the logit is 0 (log(1) = 0).
- If successes &gt; failures, the ratio &gt; 1 and the logit has a positive value (log(2) = 0.69).
- If successes &lt; failures, the ratio &lt; 1 and the logit has a negative value (log(.5) = -0.69).
]

--

&gt; Here, the estimated intercept coefficient is positive, which means that there are more successes than failures overall.

---
# GAM using other distributions

.comment[What does the smooth term indicate?]


```
#            edf   Ref.df   Chi.sq p-value
# s(x1) 4.750198 5.791279 799.8463       0
```

This represents how the log odds of successes vs failures change over time (x1).

--

&gt; As the **EDF** &gt; 1, the proportion of successes increases faster over time


```r
plot(prop_model)
```

&lt;img src="workshop08-pres-en_files/figure-html/unnamed-chunk-120-1.png" width="309.6" style="display: block; margin: auto;" /&gt;


---
# Visualizing the trend over time

There are different ways this relationship can be represented graphically:

- **partial effects** are the isolated effects of one particular predictor or interaction. If you visualize your GAM model with `plot()`, you get the partial effects.
- **summed effects** are the predicted response measures for a given value or level of predictors. If you visualize your GAM model with `itsadug::plot_smooth()`, you get the summed effects.


---
# Visualizing the trend over time

What do these plots tell us about successes vs failures?

&lt;img src="workshop08-pres-en_files/figure-html/unnamed-chunk-121-1.png" width="648" style="display: block; margin: auto;" /&gt;



.pull-left[
**Contribution / partial effect**

Over time the log odds increases, so over time successes increase and failures decrease.]

.pull-right[
**Fitted values, summed effect, intercept included**

Equal amounts of successes and failures up to x1=400.
]

&lt;!-- this code should be shown but it is not well explained...--&gt;
---
# Visualizing the trend over time

Lastly, to help interpret the results, we could transform the summed effects back to proportions with the function `itsadug::plot_smooth()`:


```r
plot_smooth(prop_model, view = "x1", main = "",
            transform = plogis, ylim = c(0,1), print.summary = F)
abline(h = 0.5, v = diff$start, col = 'red', lty = 2)
```

&lt;img src="workshop08-pres-en_files/figure-html/fig.width==4-1.png" width="432" style="display: block; margin: auto;" /&gt;

As in the logit plot, the proportion of successes increases above 0.5 at x1=400.

&lt;!-- again, lack of explanation here...--&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="qcbsR-macros.js"></script>
<script>var slideshow = remark.create({
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>

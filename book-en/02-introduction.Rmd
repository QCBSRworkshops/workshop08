# The linear model...and where it fails {#intro-linear-models}

```{r, eval=TRUE,echo = FALSE}
library(ggplot2, quietly = TRUE)
library(mgcv, quietly = TRUE)
```

What do we mean by the linear model? Regression is the workhorse of
statistics. It allows us to model a response variable as a function of
predictors plus error. Linear regression is what most people first
encounter in statistics. As we saw in Workshop 4: [Linear
models](https://github.com/QCBSRworkshops/workshop04), linear regression relies on four
major assumptions:

1.  There is a linear relationship between response and predictor variables: $y_i = \beta_0 + \beta_1 \times x_i + \epsilon_i$
2.  The error is normally distributed: $\epsilon_i \sim \mathcal{N}(0,\,\sigma^2)$
3.  The variance of the error is homogeneous (homoscedastic)
4.  The errors are independent of each other

A linear model can sometimes accommodate certain types of non-linear responses (e.g. $x^2$), but this approach strongly relies on decisions that can be either arbitrary or well-informed, and is much less flexible than using an additive model. For example, this linear model with multiple predictors can handle a non-linear response, but quickly becomes difficult to interpret and manage:

$$y_i = \beta_0 + \beta_1x_{1,i}+\beta_2x_{2,i}+\beta_3x_{3,i}+...+\beta_kx_{k,i} + \epsilon_i$$
Linear models work very well in certain specific cases where all these criteria are met:

![](images/linreg.png) 

In reality, we often cannot meet these criteria. This means that in many cases, linear models are inappropriate:

![](images/linreg_bad.png)

So, how can we fit a better model? To answer this question, we must first consider what the regression model is
trying to do. The linear model is trying to fit the best __straight line__ that passes through the middle of the data, _without __overfitting___ the data, which is what would happen if we simply drew a line between each point and its neighbours. Linear models do this by finding the best fit straight line that passes through the data. 

In the same way, additive models fit a curve through the data, while controlling the ___wiggliness___ of this curve to avoid overfitting. This means additive models like GAMs can capture non-linear relationships by fitting a smooth function through the data, rather than a straight line. We will come back to the concept of ___wiggliness___ later!

# Introduction to GAMs

Let us use an example to demonstrate the difference between a linear regression and an additive model.

First, we will load the `ISIT` dataset. This dataset is comprised of bioluminescence levels (`Sources`) in relation to depth, seasons and different stations.

```{r}
isit <- read.csv("data/ISIT.csv")
head(isit)
```

For now, we will be focusing on Season 2.

```{r}
isit2 <- subset(isit, Season == 2)
```

Let's begin by trying to fit a linear regression model to the relationship between `Sources` and `SampleDepth`. We can use the `gam()` command from the `mgcv` package here to model an ordinary least squares regression. We will see below how to use `gam()` to specify a smoothed, non-linear term.

```{r}
linear_model <- gam(Sources ~ SampleDepth, data = isit2)
summary(linear_model)
```

The linear model is explaining quite a bit of variance in our dataset ($R_{adj}$ = 0.588), which means it's a pretty good model, right? Well, let's take a look at how our model fits the data:

```{r}
data_plot <- ggplot(data = isit2, aes(y = Sources, x = SampleDepth)) + 
  geom_point() +
  geom_line(aes(y = fitted(linear_model)),
            colour = "red", size = 1.2) + 
  theme_bw()
data_plot
```

Are the assumptions of linear regression listed in [Chapter 2](#intro-linear-models) met in this case? As you may have noticed, we are violating the assumptions of the linear model:

1.  There is a strong _non-linear_ relationship between `Sources` and `SampleDepth`.
2.  The error is _not_ normally distributed.
3.  The variance of the error is _not_ homoscedastic.
4.  The errors are _not_ independent of each other.

As we mentioned briefly in [Chapter 2](#intro-linear-models), we could manually specify a linear model with multiple predictor variables to try to accommodate this non-linear response. For example, we could try to build this linear model with multiple predictors:

$$y_i = \beta_0 + \beta_1(x_{1,i}) + \beta_2(x_{2,i}) + ... + \epsilon$$
However, the fit of this model would be determined manually based on the modeller's decisions, and would quickly become difficult to work with. One big advantage of using an additive model, such as a GAM,  is that the fitting method (usually maximum likelihood) automatically determines the optimal shape of the curve fit for non-linear responses. This optimal shape is, in other words,  *the degree of smoothness* of $f(x)$.

:::explanation

Briefly, GAMs are effectively a nonparametric form of regression where the $\beta x_i$ of a linear regression is replaced by a smooth function of the explanatory variables, $f(x_i)$, and the model becomes:

$$y_i = f(x_i) + \epsilon_i$$
where $y_i$ is the response variable, $x_i$ is the predictor variable, and $f$ is the smooth function.

Importantly, given that the smooth function $f(x_i)$ is non-linear and
local, the magnitude of the effect of the explanatory variable can vary
over its range, depending on the relationship between the variable and
the response. 

That is, as opposed to one fixed coefficient $\beta$, the function $f$ can continually change over the range of $x_i$.
The degree of smoothness (or wiggliness) of $f$ is controlled using penalized regression determined automatically in `mgcv` using a generalized cross-validation (GCV) routine [@wood_2006].
:::

We can try to build a more appropriate model by fitting the data with a smoothed (non-linear) term. In `mgcv::gam()`, smooth terms are specified by expressions of the form `s(x)`, where $x$ is the non-linear predictor variable we want to smooth. In this case, we want to apply a smooth function to `SampleDepth`.

```{r}
gam_model <- gam(Sources ~ s(SampleDepth), data = isit2)
summary(gam_model)
```

The variance explained by our model has increased by more than 20% ($R_{adj}$ = 0.81)! When we compare the fit of the linear (red) and non-linear (blue) models, it is clear that the latter is more appropriate for our dataset:

```{r}
data_plot <- data_plot +
     geom_line(aes(y = fitted(gam_model)),
               colour = "blue", size = 1.2)
data_plot
```

:::explanation

Recall: As opposed to one fixed coefficient $\beta$ in a linear model, the function $f$ can vary across the range of $x_i$.

:::

The `mgcv` package also includes a default plot to look at the smooths:

```{r}
plot(gam_model)
```


## Test for linearity

How do we test whether the non-linear model offers a significant improvement over the linear model?

We can use `gam()` and `AIC()` to test whether an assumption of linearity is justified. To do so, we can compare the performance of a linear model containing `x` as a linear predictor to the performance of a non-linear model containing `s(x)` as a smooth predictor. In other words, we ask whether adding a smooth function to the linear model improves the fit of the model to our dataset.

```{r}
linear_model <- gam(Sources ~ SampleDepth, data = isit2)
smooth_model <- gam(Sources ~ s(SampleDepth), data = isit2)
AIC(linear_model, smooth_model)
```

Here, the AIC of the smooth GAM is lower, which indicates that adding a smoothing function improves model performance. Linearity is therefore not supported by our data.

:::explanation
As a brief explanation, the Akaike Information Criterion (AIC) is a comparative metric of model performance, where lower scores indicate that a model is performing "better" compared to other considered models. 
:::

## Challenge 1

We will now try to determine whether this the data recorded in the first season should be modelled with a linear regression, or with an additive model. Let's repeat the comparison test with `gam()` and `AIC()` using the data recorded in the first season only:

```{r}
isit1 <- subset(isit, Season == 1)
```

1. Fit a linear and smoothed GAM model to the relation between `SampleDepth` and `Sources`.
2. Determine if linearity is justified for this data.
3. How many effective degrees of freedom does the smoothed term have?

:::explanation
We have not discussed effective degrees of freedom (**EDF**) yet, but these are a key tool to help us interpret the fit of a GAM. Keep this term in mind. More on this in the next sections!
:::

### Challenge 1: Solution

__1.__ Fit a linear and smoothed GAM model to the relation between `SampleDepth` and `Sources`.

```{r}
linear_model_s1 <- gam(Sources ~ SampleDepth, data = isit1)
smooth_model_s1 <- gam(Sources ~ s(SampleDepth), data = isit1)
```

__2.__ Determine whether a linear model is appropriate for this data.

As before, visualizing the model fit on our dataset is an excellent first step to determine whether our model is performing well.

```{r}
ggplot(isit1, aes(x = SampleDepth, y = Sources)) +
  geom_point() +
  geom_line(colour = "red", size = 1.2,
            aes(y = fitted(linear_model_s1))) +
  geom_line(colour = "blue", size = 1.2,
            aes(y = fitted(smooth_model_s1))) +
  theme_bw()
```

We can supplement this with a quantitative comparison of model performance using `AIC()`. 

```{r}
AIC(linear_model_s1, smooth_model_s1)
```

The lower AIC score indicates that smooth model is performing better than the linear model, which confirms that linearity is not appropriate for our dataset. 

__3.__ How many effective degrees of freedom does the smoothed term have?

To get the effective degrees of freedom, we can simply print the model object:

```{r}
smooth_model_s1
```

The effective degrees of freedom (EDF) are >> 1. Keep this in mind, because we will be coming back to EDF soon!


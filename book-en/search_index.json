[["index.html", "Workshop 8: Generalized additive models in R QCBS R Workshop Series Preface 0.1 Code of conduct 0.2 Contributors 0.3 Contributing", " Workshop 8: Generalized additive models in R QCBS R Workshop Series Developed and maintained by the contributors of the QCBS R Workshop Series1 2022-02-24 16:59:33 Preface The QCBS R Workshop Series is a series of 10 workshops that walks participants through the steps required to use R for a wide array of statistical analyses relevant to research in biology and ecology. These open-access workshops were created by members of the QCBS both for members of the QCBS and the larger community. The content of this workshop has been peer-reviewed by several QCBS members. If you would like to suggest modifications, please contact the current series coordinators, listed on the main Github page. 0.1 Code of conduct The QCBS R Workshop Series and the QCBS R Symposium are venues dedicated to providing a welcoming and supportive environment for all people, regardless of background or identity. Participants, presenters and organizers of the workshop series and other related activities accept this Code of Conduct when being present at any workshop-related activities. We do not tolerate behaviour that is disrespectful or that excludes, intimidates, or causes discomfort to others. We do not tolerate discrimination or harassment based on characteristics that include, but are not limited to, gender identity and expression, sexual orientation, disability, physical appearance, body size, citizenship, nationality, ethnic or social origin, pregnancy, familial status, genetic information, religion or belief (or lack thereof), membership of a national minority, property, age, education, socio-economic status, technical choices, and experience level. It applies to all spaces managed by or affiliated with the workshop, including, but not limited to, workshops, email lists, and online forums such as GitHub, Slack and Twitter. 0.1.1 Expected behaviour All participants are expected to show respect and courtesy to others. All interactions should be professional regardless of platform: either online or in-person. In order to foster a positive and professional learning environment we encourage the following kinds of behaviours in all workshop events and platforms: Use welcoming and inclusive language Be respectful of different viewpoints and experiences Gracefully accept constructive criticism Focus on what is best for the community Show courtesy and respect towards other community members 0.1.2 Unacceptable behaviour Examples of unacceptable behaviour by participants at any workshop event/platform include: written or verbal comments which have the effect of excluding people on the - basis of membership of any specific group; causing someone to fear for their safety, such as through stalking or intimidation; violent threats or language directed against another person; the display of sexual or violent images; unwelcome sexual attention; nonconsensual or unwelcome physical contact; insults or put-downs; sexist, racist, homophobic, transphobic, ableist, or exclusionary jokes; incitement to violence, suicide, or self-harm; continuing to initiate interaction (including photography or recording) with - someone after being asked to stop; publication of private communication without consent. 0.2 Contributors This workshop was originally developed by Eric Pedersen and Zofia Taranu, and originally revised in French by Cédric Frenette Dussault. Since 2014, several QCBS members contributed to consistently and collaboratively develop and update this workshop, as part of the Learning and Development Award from the Québec Centre for Biodiversity Science. They were: Daniel Schoenig, Laurie Maynard, Marie-Hélène Brice, Kevin Cazelles, Pedro Henrique P. Braga, Esteban Gongora, Linley Sherin, Eric Pedersen, Zofia Taranu, Cédric Frenette Dussault, Emmanuelle Chrétien, Vincent Fugère. 0.3 Contributing The QCBS R Workshop Series is part of the Québec Centre for Biodiversity Science, and is maintained by the series coordinators and graduent student, postdoctoral, and research professional members. The contributors for this workshop can be accessed here.↩︎ "],["learning-objectives.html", "Chapter 1 Learning objectives", " Chapter 1 Learning objectives The goal of Workhsop 8 is to first examine what we mean by a non-linear model, and how Generalized Additive Models (GAMs) allow us to handle non-linear relationships. We will also go over how to plot and interpret these non-linear relationships, how to include interaction terms, autocorrelated errors, and expand on previous workshops by briefly examining a mixed modelling framework. Lastly, we will briefly touch upon what GAMs are doing behind the scenes. We recommend some working experience in R, particularly with examining data and objects in R scripts, and a basic knowledge of linear regression before following this workshop. More specifically, this workshop will cover how to: Use the mgcv package to fit non-linear relationships, Understand the output of a GAM to help you understand your data, Use tests to determine if a non-linear model fits better than a linear one, Include smooth interactions between variables, Understand the idea of a basis function, and why it makes GAMs so powerful, Account for dependence in data (autocorrelation, hierarchical structure) using GAMMs. "],["preparing-for-the-workshop.html", "Chapter 2 Preparing for the workshop", " Chapter 2 Preparing for the workshop All workshop materials are found at r.qcbs.ca/workshops/r-workshop-08. This includes an R script which contains all code chunks shown in this book. For this workshop, we will be working with the following datasets: ISIT.csv other_dist.csv You should also make sure you have downloaded, installed, and loaded these packages: ggplot2 itsadug mgcv install.packages(&quot;ggplot2&quot;) install.packages(&quot;mgcv&quot;) install.packages(&quot;itsadug&quot;) library(ggplot2) library(mgcv) library(itsadug) "],["intro-linear-models.html", "Chapter 3 The linear model… and where it fails", " Chapter 3 The linear model… and where it fails What do we mean by the linear model? Regression is the workhorse of statistics. It allows us to model a response variable as a function of predictors plus error. Linear regression is what most people first encounter in statistics. As we saw in Workshop 4: Linear models, linear regression relies on four major assumptions: There is a linear relationship between response and predictor variables: \\(y_i = \\beta_0 + \\beta_1 \\times x_i + \\epsilon_i\\) The error is normally distributed: \\(\\epsilon_i \\sim \\mathcal{N}(0,\\,\\sigma^2)\\) The variance of the error is homogeneous (homoscedastic) The errors are independent of each other A linear model can sometimes accommodate certain types of non-linear responses (e.g. \\(x^2\\)), but this approach strongly relies on decisions that can be either arbitrary or well-informed, and is much less flexible than using an additive model. For example, this linear model with multiple predictors can handle a non-linear response, but quickly becomes difficult to interpret and manage: \\[y_i = \\beta_0 + \\beta_1x_{1,i}+\\beta_2x_{2,i}+\\beta_3x_{3,i}+...+\\beta_kx_{k,i} + \\epsilon_i\\] Linear models work very well in certain specific cases where all these criteria are met: In reality, we often cannot meet these criteria. This means that in many cases, linear models are inappropriate: So, how can we fit a better model? To answer this question, we must first consider what the regression model is trying to do. The linear model is trying to fit the best straight line that passes through the middle of the data, without overfitting the data, which is what would happen if we simply drew a line between each point and its neighbours. Linear models do this by finding the best fit straight line that passes through the data. In the same way, additive models fit a curve through the data, while controlling the wiggliness of this curve to avoid overfitting. This means additive models like GAMs can capture non-linear relationships by fitting a smooth function through the data, rather than a straight line. We will come back to the concept of wiggliness later! "],["introduction-to-gams.html", "Chapter 4 Introduction to GAMs 4.1 Test for linearity 4.2 Challenge 1", " Chapter 4 Introduction to GAMs Let us use an example to demonstrate the difference between a linear regression and an additive model. First, we will load the ISIT dataset. This dataset is comprised of bioluminescence levels (Sources) in relation to depth, seasons and different stations. isit &lt;- read.csv(&quot;data/ISIT.csv&quot;) head(isit) ## SampleDepth Sources Station Time Latitude Longitude Xkm Ykm Month Year ## 1 517 28.73 1 3 50.1508 -14.4792 -34.106 16.779 4 2001 ## 2 582 27.90 1 3 50.1508 -14.4792 -34.106 16.779 4 2001 ## 3 547 23.44 1 3 50.1508 -14.4792 -34.106 16.779 4 2001 ## 4 614 18.33 1 3 50.1508 -14.4792 -34.106 16.779 4 2001 ## 5 1068 12.38 1 3 50.1508 -14.4792 -34.106 16.779 4 2001 ## 6 1005 11.23 1 3 50.1508 -14.4792 -34.106 16.779 4 2001 ## BottomDepth Season Discovery RelativeDepth ## 1 3939 1 252 3422 ## 2 3939 1 252 3357 ## 3 3939 1 252 3392 ## 4 3939 1 252 3325 ## 5 3939 1 252 2871 ## 6 3939 1 252 2934 For now, we will be focusing on Season 2. isit2 &lt;- subset(isit, Season == 2) Let’s begin by trying to fit a linear regression model to the relationship between Sources and SampleDepth. We can use the gam() command from the mgcv package here to model an ordinary least squares regression. We will see below how to use gam() to specify a smoothed, non-linear term. linear_model &lt;- gam(Sources ~ SampleDepth, data = isit2) summary(linear_model) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## Sources ~ SampleDepth ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30.9021874 0.7963891 38.80 &lt;2e-16 *** ## SampleDepth -0.0083450 0.0003283 -25.42 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## R-sq.(adj) = 0.588 Deviance explained = 58.9% ## GCV = 60.19 Scale est. = 59.924 n = 453 The linear model is explaining quite a bit of variance in our dataset (\\(R_{adj}\\) = 0.588), which means it’s a pretty good model, right? Well, let’s take a look at how our model fits the data: data_plot &lt;- ggplot(data = isit2, aes(y = Sources, x = SampleDepth)) + geom_point() + geom_line(aes(y = fitted(linear_model)), colour = &quot;red&quot;, size = 1.2) + theme_bw() data_plot Are the assumptions of linear regression listed in Chapter 2 met in this case? As you may have noticed, we are violating the assumptions of the linear model: There is a strong non-linear relationship between Sources and SampleDepth. The error is not normally distributed. The variance of the error is not homoscedastic. The errors are not independent of each other. As we mentioned briefly in Chapter 4, we could manually specify a linear model with multiple predictor variables to try to accommodate this non-linear response. For example, we could try to build this linear model with multiple predictors: \\[y_i = \\beta_0 + \\beta_1(x_{1,i}) + \\beta_2(x_{2,i}) + ... + \\epsilon\\] However, the fit of this model would be determined manually based on the modeller’s decisions, and would quickly become difficult to work with. One big advantage of using an additive model, such as a GAM, is that the fitting method (usually maximum likelihood) automatically determines the optimal shape of the curve fit for non-linear responses. This optimal shape is, in other words, the degree of smoothness of \\(f(x)\\). Briefly, GAMs are effectively a nonparametric form of regression where the \\(\\beta x_i\\) of a linear regression is replaced by a smooth function of the explanatory variables, \\(f(x_i)\\), and the model becomes: \\[y_i = f(x_i) + \\epsilon_i\\] where \\(y_i\\) is the response variable, \\(x_i\\) is the predictor variable, and \\(f\\) is the smooth function. Importantly, given that the smooth function \\(f(x_i)\\) is non-linear and local, the magnitude of the effect of the explanatory variable can vary over its range, depending on the relationship between the variable and the response. That is, as opposed to one fixed coefficient \\(\\beta\\), the function \\(f\\) can continually change over the range of \\(x_i\\). The degree of smoothness (or wiggliness) of \\(f\\) is controlled using penalized regression determined automatically in mgcv using a generalized cross-validation (GCV) routine (Wood 2006). We can try to build a more appropriate model by fitting the data with a smoothed (non-linear) term. In mgcv::gam(), smooth terms are specified by expressions of the form s(x), where \\(x\\) is the non-linear predictor variable we want to smooth. In this case, we want to apply a smooth function to SampleDepth. gam_model &lt;- gam(Sources ~ s(SampleDepth), data = isit2) summary(gam_model) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## Sources ~ s(SampleDepth) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 12.8937 0.2471 52.17 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(SampleDepth) 8.908 8.998 214.1 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.81 Deviance explained = 81.4% ## GCV = 28.287 Scale est. = 27.669 n = 453 The variance explained by our model has increased by more than 20% (\\(R_{adj}\\) = 0.81)! When we compare the fit of the linear (red) and non-linear (blue) models, it is clear that the latter is more appropriate for our dataset: data_plot &lt;- data_plot + geom_line(aes(y = fitted(gam_model)), colour = &quot;blue&quot;, size = 1.2) data_plot Recall: As opposed to one fixed coefficient \\(\\beta\\) in a linear model, the function \\(f\\) can vary across the range of \\(x_i\\). The mgcv package also includes a default plot to look at the smooths: plot(gam_model) 4.1 Test for linearity How do we test whether the non-linear model offers a significant improvement over the linear model? We can use gam() and AIC() to test whether an assumption of linearity is justified. To do so, we can compare the performance of a linear model containing x as a linear predictor to the performance of a non-linear model containing s(x) as a smooth predictor. In other words, we ask whether adding a smooth function to the linear model improves the fit of the model to our dataset. linear_model &lt;- gam(Sources ~ SampleDepth, data = isit2) smooth_model &lt;- gam(Sources ~ s(SampleDepth), data = isit2) AIC(linear_model, smooth_model) ## df AIC ## linear_model 3.00000 3143.720 ## smooth_model 10.90825 2801.451 Here, the AIC of the smooth GAM is lower, which indicates that adding a smoothing function improves model performance. Linearity is therefore not supported by our data. As a brief explanation, the Akaike Information Criterion (AIC) is a comparative metric of model performance, where lower scores indicate that a model is performing “better” compared to other considered models. 4.2 Challenge 1 We will now try to determine whether this the data recorded in the first season should be modelled with a linear regression, or with an additive model. Let’s repeat the comparison test with gam() and AIC() using the data recorded in the first season only: isit1 &lt;- subset(isit, Season == 1) Fit a linear and smoothed GAM model to the relation between SampleDepth and Sources. Determine if linearity is justified for this data. How many effective degrees of freedom does the smoothed term have? We have not discussed effective degrees of freedom (EDF) yet, but these are a key tool to help us interpret the fit of a GAM. Keep this term in mind. More on this in the next sections! 4.2.1 Challenge 1: Solution 1. Fit a linear and smoothed GAM model to the relation between SampleDepth and Sources. linear_model_s1 &lt;- gam(Sources ~ SampleDepth, data = isit1) smooth_model_s1 &lt;- gam(Sources ~ s(SampleDepth), data = isit1) 2. Determine whether a linear model is appropriate for this data. As before, visualizing the model fit on our dataset is an excellent first step to determine whether our model is performing well. ggplot(isit1, aes(x = SampleDepth, y = Sources)) + geom_point() + geom_line(colour = &quot;red&quot;, size = 1.2, aes(y = fitted(linear_model_s1))) + geom_line(colour = &quot;blue&quot;, size = 1.2, aes(y = fitted(smooth_model_s1))) + theme_bw() We can supplement this with a quantitative comparison of model performance using AIC(). AIC(linear_model_s1, smooth_model_s1) ## df AIC ## linear_model_s1 3.000000 2324.905 ## smooth_model_s1 9.644938 2121.249 The lower AIC score indicates that smooth model is performing better than the linear model, which confirms that linearity is not appropriate for our dataset. 3. How many effective degrees of freedom does the smoothed term have? To get the effective degrees of freedom, we can simply print the model object: smooth_model_s1 ## ## Family: gaussian ## Link function: identity ## ## Formula: ## Sources ~ s(SampleDepth) ## ## Estimated degrees of freedom: ## 7.64 total = 8.64 ## ## GCV score: 32.13946 The effective degrees of freedom (EDF) are &gt;&gt; 1. Keep this in mind, because we will be coming back to EDF later! References "],["how-gams-work.html", "Chapter 5 How GAMs work 5.1 Example: a polynomial basis 5.2 Example: a cubic spline basis", " Chapter 5 How GAMs work We will now take a few minutes to look at what GAMs are doing behind the scenes. Let us first consider a simple model containing one smooth function \\(f\\) of one covariate, \\(x\\): \\[y_i = f(x_i) + \\epsilon_i\\] To estimate the smooth function \\(f\\), we need to represented the above equation in such a way that it becomes a linear model. This can be done by defining basis functions, \\(b_j(x)\\), of which \\(f\\) is composed: \\[f(x) = \\sum_{j=1}^q b_j(x) \\times \\beta_j\\] 5.1 Example: a polynomial basis Suppose that \\(f\\) is believed to be a 4th order polynomial, so that the space of polynomials of order 4 and below contains \\(f\\). A basis for this space would then be: \\[b_0(x)=1 \\ , \\quad b_1(x)=x \\ , \\quad b_2(x)=x^2 \\ , \\quad b_3(x)=x^3 \\ , \\quad b_4(x)=x^4\\] so that \\(f(x)\\) becomes: \\[f(x) = \\beta_0 + x\\beta_1 + x^2\\beta_2 + x^3\\beta_3 + x^4\\beta_4\\] and the full model now becomes: \\[y_i = \\beta_0 + x_i\\beta_1 + x^2_i\\beta_2 + x^3_i\\beta_3 + x^4_i\\beta_4 + \\epsilon_i\\] The basis functions are each multiplied by a real valued parameter, \\(\\beta_j\\), and are then summed to give the final curve \\(f(x)\\). By varying the \\(\\beta_j\\) we can vary the form of \\(f(x)\\) to produce any polynomial function of order 4 or lower. 5.2 Example: a cubic spline basis A cubic spline is a curve constructed from sections of a cubic polynomial joined together so that they are continuous in value. Each section of cubic has different coefficients. Here is a representation of a smooth function using a rank 5 cubic spline basis with knot locations at increments of 0.2: Here, the knots are evenly spaced through the range of observed x values. However, the choice of the degree of model smoothness is controlled by the the number of knots, which was arbitrary. Is there a better way to select the knot locations? 5.2.1 Controlling the degree of smoothing with penalized regression splines Instead of controlling smoothness by altering the number of knots, we keep that fixed to size a little larger than reasonably necessary, and control the model’s smoothness by adding a “wiggleness” penalty. So, rather than fitting the model by minimizing (as with least squares regression): \\[||y - XB||^{2}\\] it can be fit by minimizing: \\[||y - XB||^{2} + \\lambda \\int_0^1[f^{&#39;&#39;}(x)]^2dx\\] Where, as \\(\\lambda\\) goes to \\(\\infty\\), the model becomes linear. The selection of the best fit smoothing parameter, \\(\\lambda\\), makes use of a cross-validation approach. If \\(\\lambda\\) is too high then the data will be over smoothed, and if it is too low, the data will be under smoothed. Ideally, it is best to choose \\(\\lambda\\) so that the \\(\\hat{f}\\) is as close as possible to \\(f\\). A suitable criterion might be to choose \\(\\lambda\\) to minimize: \\[M = 1/n \\times \\sum_{i=1}^n (\\hat{f_i} - f_i)^2\\] Since \\(f\\) is unknown, \\(M\\) must be estimated. The recommend methods for this are maximum likelihood (ML) or restricted maximum likelihood estimation (REML). Generalized cross validation (GCV) is another possibility, which is a technique that leaves out each datum from the data in turn and considers the average predictive ability of models fitted to the remaining data to predict the removed datum. For further details on these methods, see (Wood 2006). The principle behind cross validation In the first panel, \\(\\lambda\\) is too high, and the curve is oversmoothed. Here, the curve loosely fits the data and therefore predicts the missing point very poorly. In the third panel, \\(\\lambda\\) is too high, and the curver is overfitted. Here, the curve fits the data very closely, following the signal as well as the noise surrounding it. The influence of this additional (uninformative) variability causes it to predict the missing datum rather poorly. In the second panel, \\(\\lambda\\) is just about right. The curve fits the underlying signal quite well, while smoothing through the noise. The missing datum is reasonably well predicted. References "],["gam-with-multiple-smooth-terms.html", "Chapter 6 GAM with multiple smooth terms 6.1 GAM with linear and smooth terms 6.2 Effective degrees of freedom (EDF) 6.3 GAM with multiple linear and smooth terms 6.4 GAM with multiple smooth terms 6.5 Challenge 2", " Chapter 6 GAM with multiple smooth terms 6.1 GAM with linear and smooth terms GAMs make it easy to include both smooth and linear terms, multiple smoothed terms, and smoothed interactions. For this section, we will use the ISIT dataset again. We will try to model the response Sources using the predictors Season and SampleDepth simultaneously. Remember this dataset from previous sections? The ISIT dataset is comprised of bioluminescence levels (Sources) in relation to depth, seasons and different stations. First, we need to convert our categorical predictor (Season) into a factor variable. head(isit) ## SampleDepth Sources Station Time Latitude Longitude Xkm Ykm Month Year ## 1 517 28.73 1 3 50.1508 -14.4792 -34.106 16.779 4 2001 ## 2 582 27.90 1 3 50.1508 -14.4792 -34.106 16.779 4 2001 ## 3 547 23.44 1 3 50.1508 -14.4792 -34.106 16.779 4 2001 ## 4 614 18.33 1 3 50.1508 -14.4792 -34.106 16.779 4 2001 ## 5 1068 12.38 1 3 50.1508 -14.4792 -34.106 16.779 4 2001 ## 6 1005 11.23 1 3 50.1508 -14.4792 -34.106 16.779 4 2001 ## BottomDepth Season Discovery RelativeDepth ## 1 3939 1 252 3422 ## 2 3939 1 252 3357 ## 3 3939 1 252 3392 ## 4 3939 1 252 3325 ## 5 3939 1 252 2871 ## 6 3939 1 252 2934 isit$Season &lt;- as.factor(isit$Season) Let us start with a basic model, with one smoothed term (SampleDepth) and one categorical predictor (Season, which has 2 levels). basic_model &lt;- gam(Sources ~ Season + s(SampleDepth), data = isit, method = &quot;REML&quot;) basic_summary &lt;- summary(basic_model) The p.table provides information on the linear effects: basic_summary$p.table ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.253273 0.3612666 20.07734 1.430234e-72 ## Season2 6.156130 0.4825491 12.75752 5.525673e-34 The s.table provides information on the smooth (non-linear) effects: basic_summary$s.table ## edf Ref.df F p-value ## s(SampleDepth) 8.706426 8.975172 184.3583 0 The edf shown in the s.table are the effective degrees of freedom (EDF) of the the smooth term s(SampleDepth). Essentially, more EDF imply more complex, wiggly splines. When a term has an EDF value that is close to 1, it is close to being a linear term. Higher values indicate that the term’s spline is more wiggly, or in other words, highly non-linear. In our basic model, the EDF of the smooth function s(SampleDepth) are ~9, which suggests a highly non-linear curve. Let us plot the smoothed (s(SampleDepth)) and the linear (Season) terms from our fitted model: plot(basic_model, all.terms = TRUE, page = 1) What do these plots tell us about the relationship between bioluminescence, sample depth, and seasons? Bioluminescence varies non-linearly across the SampleDepth gradient, with highest levels of bioluminescence at the surface, followed by a second but smaller peak just above a depth of 1500, with declining levels at deeper depths. There is also a pronounced difference in bioluminescence between the seasons, with high levels during Season 2, compared to Season 1. 6.2 Effective degrees of freedom (EDF) Let us come back to the concept of effective degrees of freedom (EDF). Effective degrees of freedom give us a lot of information about the relationship between model predictors and response variables. You might recognize the term “degrees of freedom” from previous workshops about linear models, but be careful! The effective degrees of freedom of a GAM are estimated differently from the degrees of freedom in a linear regression, and are interpreted differently. In linear regression, the model degrees of freedom are equivalent to the number of non-redundant free parameters \\(p\\) in the model, and the residual degrees of freedom are given by \\(n-p\\). Because the number of free parameters in GAMs is difficult to define, the EDF are instead related to the smoothing parameter \\(\\lambda\\), such that the greater the penalty, the smaller the EDF. An upper bound on the EDF is determined by the basis dimension \\(k\\) for each smooth function, meaning the EDF cannot exceed \\(k-1\\). In practice, the exact choice of \\(k\\) is arbitrary, but it should be large enough to accommodate a sufficiently complex smooth function. We will talk about choosing \\(k\\) in Chapter 6. Higher EDF imply more complex, wiggly splines. When a term has an EDF value that is close to 1, it is close to being a linear term. Higher values indicate that the term is more wiggly, or in other words, more non-linear! 6.3 GAM with multiple linear and smooth terms We can add a second term (RelativeDepth) to our basic model, but specify a linear relationship with Sources. two_term_model &lt;- gam(Sources ~ Season + s(SampleDepth) + RelativeDepth, data = isit, method = &quot;REML&quot;) two_term_summary &lt;- summary(two_term_model) The regression coefficient which is estimated for this new linear term, RelativeDepth, will appear in the p.table. Remember, the p.table shows information on the parametric effects (linear terms): two_term_summary$p.table ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.808305503 0.6478741951 15.139213 1.446613e-45 ## Season2 6.041930627 0.4767977508 12.671894 1.380010e-33 ## RelativeDepth -0.001401908 0.0002968443 -4.722705 2.761048e-06 In the s.table, we will once again find the non-linear smoother, s(SampleDepth), and its wiggleness parameter (edf). Remember, the s.table shows information on the additive effects (non-linear terms): two_term_summary$s.table ## edf Ref.df F p-value ## s(SampleDepth) 8.699146 8.97396 132.4801 0 Let us take a look at the relationships between the linear and non-linear predictors and our response variable. plot(two_term_model, page = 1, all.terms = TRUE) 6.4 GAM with multiple smooth terms If we want to know whether the relationship between Sources and RelativeDepth is non-linear, we can model RelativeDepth as a smooth term instead. In this model, we would have two smooth terms: two_smooth_model &lt;- gam(Sources ~ Season + s(SampleDepth) + s(RelativeDepth), data = isit, method = &quot;REML&quot;) two_smooth_summary &lt;- summary(two_smooth_model) The regression coefficient which is estimated for our only linear term, Season, will appear in the p.table. Remember, the p.table shows information on the parametric effects (linear terms): two_smooth_summary$p.table ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.937755 0.3452945 22.98836 1.888513e-89 ## Season2 4.963951 0.4782280 10.37988 1.029016e-23 In the s.table, we will now find two non-linear smoothers, s(SampleDepth) and s(RelativeDepth), and their wiggleness parameters (edf). Remember, the s.table shows information on the additive effects (non-linear terms): two_smooth_summary$s.table ## edf Ref.df F p-value ## s(SampleDepth) 8.752103 8.973459 150.37263 0 ## s(RelativeDepth) 8.044197 8.749580 19.97476 0 Let us take a look at the relationships between the linear and non-linear predictors and our response variable. plot(two_smooth_model, page = 1, all.terms = TRUE) Do you think that the additional non-linear term improves the performance of our model representing the relationship between bioluminescence and relative depth? As before, we can compare our models with AIC to test whether the smoothed term improves our model’s performance: AIC(basic_model, two_term_model, two_smooth_model) ## df AIC ## basic_model 11.83374 5208.713 ## two_term_model 12.82932 5188.780 ## two_smooth_model 20.46960 5056.841 We can see that two_smooth_model has the lowest AIC value. The best fit model includes both smooth terms for SampleDepth and RelativeDepth, and a linear term for Season. 6.5 Challenge 2 For our second challenge, we will be building onto our model by adding variables which we think might be ecologically significant predictors to explain bioluminescence. Create two new models: Add Latitude to two_smooth_model, first as a linear term, then as a smoothed term. Is Latitude an important term to include? Does Latitude have a linear or additive effect? Use plots, coefficient tables, and the AIC() function to help you answer this question. 6.5.1 Challenge 2: Solution 1. Create two new models: Add Latitude to two_smooth_model, first as a linear term, then as a smoothed term. # Add Latitude as a linear term three_term_model &lt;- gam(Sources ~ Season + s(SampleDepth) + s(RelativeDepth) + Latitude, data = isit, method = &quot;REML&quot;) (three_term_summary &lt;- summary(three_term_model)) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## Sources ~ Season + s(SampleDepth) + s(RelativeDepth) + Latitude ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -102.7094 40.6913 -2.524 0.01180 * ## Season2 6.0345 0.6179 9.766 &lt; 2e-16 *** ## Latitude 2.2188 0.8159 2.719 0.00669 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(SampleDepth) 8.750 8.973 92.84 &lt;2e-16 *** ## s(RelativeDepth) 8.047 8.751 16.90 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.75 Deviance explained = 75.6% ## -REML = 2545.7 Scale est. = 34.309 n = 789 # Add Latitude as a smooth term three_smooth_model &lt;- gam(Sources ~ Season + s(SampleDepth) + s(RelativeDepth) + s(Latitude), data = isit, method = &quot;REML&quot;) (three_smooth_summary &lt;- summary(three_smooth_model)) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## Sources ~ Season + s(SampleDepth) + s(RelativeDepth) + s(Latitude) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.7045 0.4716 14.215 &lt;2e-16 *** ## Season2 7.1120 0.7441 9.557 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(SampleDepth) 8.767 8.976 68.951 &lt;2e-16 *** ## s(RelativeDepth) 8.007 8.731 17.639 &lt;2e-16 *** ## s(Latitude) 7.431 8.297 8.954 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.771 Deviance explained = 77.8% ## -REML = 2524.5 Scale est. = 31.487 n = 789 2. Is Latitude an important term to include? Does Latitude have a linear or additive effect? Let us begin by plotting the the 4 effects that are now included in each model: plot(three_term_model, page = 1, all.terms = TRUE) plot(three_smooth_model, page = 1, all.terms = TRUE) We should also look at our coefficient tables. What can the EDF tell us about the wiggliness of our predictors’ effects? three_smooth_summary$s.table ## edf Ref.df F p-value ## s(SampleDepth) 8.766891 8.975682 68.950905 0 ## s(RelativeDepth) 8.007411 8.730625 17.639321 0 ## s(Latitude) 7.431116 8.296838 8.954349 0 The EDF are all quite high for our variables, including Latitude. This tells us that Latitude is quite wiggly, and probably should not be included as a linear term. Before deciding which model is “best”, we should test whether the effect of Latitude is best included as a linear or as a smooth term, using AIC(): AIC(three_smooth_model, three_term_model) ## df AIC ## three_smooth_model 28.20032 4990.546 ## three_term_model 21.47683 5051.415 Our model including Latitude as a smooth term has a lower AIC score, meaning it performs better than our model including Latitude as a linear term. But, does adding Latitude as a smooth predictor actually improve on our last “best” model (two_smooth_model)? AIC(two_smooth_model, three_smooth_model) ## df AIC ## two_smooth_model 20.46960 5056.841 ## three_smooth_model 28.20032 4990.546 Our three_smooth_model, which includes SampleDepth, RelativeDepth, and Latitude as smooth terms, and Season as a linear term, has a lower AIC score than our previous best model, which did not include Latitude. This implies that Latitude is indeed an informative non-linear predictor of bioluminescence. "],["gam-with-interaction-terms.html", "Chapter 7 GAM with interaction terms 7.1 Interaction between smoothed and factor variables 7.2 Interaction between smoothed variables", " Chapter 7 GAM with interaction terms There are two ways to include interactions between variables: For two smoothed variables, the syntax would be: s(x1, x2) For one smoothed variable and one linear variable (either factor or continuous), the syntax would use the by argument s(x1, by = x2): When x2 is a factor, you have a smooth term that vary between different levels of x2; When x2 is continuous, the linear effect of x2 varies smoothly with x1; When x2 is a factor, the factor needs to be added as a main effect in the model. 7.1 Interaction between smoothed and factor variables We will examine interaction effects to determine whether the non-linear smoother s(SampleDepth) varies across different levels of Season. factor_interact &lt;- gam(Sources ~ Season + s(SampleDepth, by = Season) + s(RelativeDepth), data = isit, method = &quot;REML&quot;) summary(factor_interact)$s.table ## edf Ref.df F p-value ## s(SampleDepth):Season1 6.839386 7.552045 95.119422 0 ## s(SampleDepth):Season2 8.744574 8.966290 154.474325 0 ## s(RelativeDepth) 6.987223 8.055898 6.821074 0 plot(factor_interact, page = 1) The first two panels show the interaction effect of the SampleDepth smooth and each level of our factor variable, Season. Do you see a difference between the two smooth curves? The plots show some differences between the shape of the smooth terms among the two levels of Season. The most notable difference is the peak in the second panel, which tells us that there is an effect of SampleDepth between 1000 and 2000 that is important in Season 2, but does not occur in Season 1. This hints that the interaction effect could be important to include in our model. We can also plot the interaction effect in 3D on a single plot, using vis.gam(). vis.gam(factor_interact, theta = 120, n.grid = 50, lwd = 0.4) &gt; This plot can be rotated by changing the value of the theta argument. To test our idea that this interaction is important, we will perform a model comparison using AIC to determine whether the interaction term improves our model’s performance. AIC(two_smooth_model, factor_interact) ## df AIC ## two_smooth_model 20.46960 5056.841 ## factor_interact 26.99693 4878.631 The AIC of our model with a factor interaction between the SampleDepth smooth and Season has a lower AIC score, which tells us this model performs better than two_smooth_model. Including this interaction seems to improve our model’s performance. 7.2 Interaction between smoothed variables Next, we’ll look at the interactions between two smoothed terms, SampleDepth and RelativeDepth. smooth_interact &lt;- gam(Sources ~ Season + s(SampleDepth, RelativeDepth), data = isit, method = &quot;REML&quot;) summary(smooth_interact)$s.table ## edf Ref.df F p-value ## s(SampleDepth,RelativeDepth) 27.12521 28.77 93.91722 0 In the previous section, we were able to visualise an interaction effect between a smooth and a factor term by plotting a different smooth function of SampleDepth for each level of Season. In this model, we have two smoothed terms, which means that the effect of SampleDepth varies smoothly with RelativeDepth, and vice-versa. When we visualise this interaction, we instead get a gradient between two continuous smooth functions: plot(smooth_interact, page = 1, scheme = 2) We can also plot this interaction on a 3D surface: vis.gam(smooth_interact, view = c(&quot;SampleDepth&quot;, &quot;RelativeDepth&quot;), theta = 50, n.grid = 50, lwd = 0.4) &gt; Remember, this plot can be rotated by changing the value of the theta argument. You can change the colour of the 3D plot using the color argument. Try specifying color = \"cm\" in vis.gam() above, and check ?vis.gam for more color options. The plots do illustrate a non-linear interaction effect, where Sources is lowest at high values of SampleDepth and RelativeDepth, but increases with RelativeDepth while SampleDepth is low. So, there does seem to be an interaction effect between these smooth terms. Does including the interaction between s(SampleDepth) and s(RelativeDepth) improve our two_smooth_model model? AIC(two_smooth_model, smooth_interact) ## df AIC ## two_smooth_model 20.46960 5056.841 ## smooth_interact 30.33625 4943.890 The model with the interaction between s(SampleDepth) and s(RelativeDepth) has a lower AIC, which means including this interaction improves our model’s performance, and our ability to understand the drivers of bioluminescence. Using other distributions for the response variable with the family argument (just as in a GLM), Using different kinds of basis functions, Using different kinds of random effects to fit mixed effect models. We will now go over these aspects. "],["gam-model-checking.html", "Chapter 8 GAM model checking 8.1 Selecting \\(k\\)", " Chapter 8 GAM model checking So far, we have worked with simple (Gaussian) additive models, the non-linear equivalent to a linear model. However, ecological datasets often do not fit the assumptions rof Gaussian models. So, what can we do if the observations of the response variable do not follow a Normal distribution? Or if the variance is not constant (heteroscedasticity)? Just like generalized linear models (GLM), we can formulate generalized additive models to deal with these issues. Let us return to our smooth interaction model for the bioluminescence data: smooth_interact &lt;- gam(Sources ~ Season + s(SampleDepth, RelativeDepth), data = isit, method = &quot;REML&quot;) summary(smooth_interact)$p.table ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.077356 0.4235432 19.070912 1.475953e-66 ## Season2 4.720806 0.6559436 7.196969 1.480113e-12 summary(smooth_interact)$s.table ## edf Ref.df F p-value ## s(SampleDepth,RelativeDepth) 27.12521 28.77 93.91722 0 As with a GLM, it is essential to check whether the model is correctly specified, especially in regards to the distribution of the response variable. We need to verify: The choice of our basis dimension, \\(k\\). The distribution of the model residuals, just as we do for a GLM (see Workshop 6). Luckily, mgcv includes helpful functions for model checking: k.check() performs a basis dimension check. gam.check() produces residual plots (and also calls k.check()). 8.1 Selecting \\(k\\) In Chapter 5, we discussed the role of the smoothing parameter \\(\\lambda\\) in controlling the wiggliness of our smoothing functions. This wiggliness is further controlled by the basis dimension \\(k\\), which sets the number of basis functions used to create a smooth function. Each smooth in a GAM essentially the weighted sum of many smaller functions, called basis functions. The more basis functions used to build a smooth function, the more wiggly the smooth. As you can see below, a smooth with a small \\(k\\) basis dimension will be less wiggly than a smooth with a high \\(k\\) basis dimension. Throughout this workshop, we have been working towards improving the fit of our model, meaning we have been trying to build the best possible GAM to capture relationships in our dataset. The key to getting a good model fit is to balance the trade-off between two things: The smoothing parameter \\(\\lambda\\), which penalizes wiggliness; The basis dimension \\(k\\), which allows the model to wiggle according to our data. Have we optimized the tradeoff between smoothness (\\(\\lambda\\)) and wiggliness (\\(k\\)) in our model? 8.1.1 Is our model wiggly enough? Although we have not yet specified a \\(k\\) value in our model, we know that the default for smooth interactions in gam() is k = 30. Is \\(k\\) large enough? k.check(smooth_interact) ## k&#39; edf k-index p-value ## s(SampleDepth,RelativeDepth) 29 27.12521 0.9448883 0.06 The EDF are very close to k. This means the wiggliness of the model is being overly constrained by the default k = 30, and could fit the data better with greater wiggliness. In other words, the tradeoff between smoothness and wiggliness is not balanced. We can refit the model with a larger k: smooth_interact_k60 &lt;- gam(Sources ~ Season + s(SampleDepth, RelativeDepth, k = 60), data = isit, method = &quot;REML&quot;) Is k large enough this time? k.check(smooth_interact_k60) ## k&#39; edf k-index p-value ## s(SampleDepth,RelativeDepth) 59 46.03868 1.048626 0.91 The EDF are much smaller than k, which means this model fits the data better with additional wiggliness. We can replace our previous model with this wigglier version: smooth_interact &lt;- smooth_interact_k60 8.1.2 Is our model actually Normal? As with any Normal model, we must check some model assumptions before continuing. We can evaluate the distribution of the model residuals to verify these assumptions, just as we would do for a GLM (see Workshop 6). We can look at the residual plots with gam.check(): gam.check(smooth_interact) ## ## Method: REML Optimizer: outer newton ## full convergence after 4 iterations. ## Gradient range [-0.0005267781,0.0001620713] ## (score 2487.848 &amp; scale 27.40287). ## Hessian positive definite, eigenvalue range [15.84516,393.7878]. ## Model rank = 61 / 61 ## ## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may ## indicate that k is too low, especially if edf is close to k&#39;. ## ## k&#39; edf k-index p-value ## s(SampleDepth,RelativeDepth) 59 46 1.05 0.9 In addition to the plots, gam.check() also provides the output of k.check(). If you would like more in-depth explanations of how to interpret residual plots, we recommend consulting Workshop 4 and Workshop 6. These residual plots highlight some problems: Panel 2: The variance of the error is not constant (heteroscedasticity). Panels 1 and 4: There are a few strong outliers in this dataset. It seems that our response variable should not be modelled with a Normal distribution! "],["other-distributions.html", "Chapter 9 Other distributions 9.1 Challenge 3", " Chapter 9 Other distributions We need a probability distribution that allows the variance to increase with the mean. One family of distributions that has this property and that works well in a GAM is the Tweedie family. A common link function for Tweedie distributions is the \\(log\\). As in a GLM, we can use the family = argument in gam() to fit models with other distributions (including distributions such as binomial, poisson, gamma etc.). To get an overview of families available in mgcv: `?`(family.mgcv) 9.1 Challenge 3 Fit a new model smooth_interact_tw with the same formula as the smooth_interact model, but with a distribution from the Tweedie family (instead of the Normal distribution) and log link function. You can do so by using family = tw(link = \"log\") inside gam(). Check the choice of k and the residual plots for the new model. Compare smooth_interact_tw with smooth_interact. Which one would you choose? As a reminder, here is our smooth_interact model: # Hint! Because the Normal distribution is the default # setting, we have not specified the distribution in this # workshop yet. # Here is how we would write the model to specify the # Normal distribution: smooth_interact &lt;- gam(Sources ~ Season + s(SampleDepth, RelativeDepth, k = 60), family = gaussian(link = &quot;identity&quot;), data = isit, method = &quot;REML&quot;) 9.1.1 Challenge 3: Solution 1. First, let us fit a new model with the Tweedie distribution and a log link function. smooth_interact_tw &lt;- gam(Sources ~ Season + s(SampleDepth, RelativeDepth, k = 60), family = tw(link = &quot;log&quot;), data = isit, method = &quot;REML&quot;) summary(smooth_interact_tw)$p.table ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.3126641 0.03400390 38.60333 8.446478e-180 ## Season2 0.5350529 0.04837342 11.06089 1.961733e-26 summary(smooth_interact_tw)$s.table ## edf Ref.df F p-value ## s(SampleDepth,RelativeDepth) 43.23949 51.57139 116.9236 0 2. Check the choice of k and the residual plots for the new model. Next, we should check the basis dimension: k.check(smooth_interact_tw) ## k&#39; edf k-index p-value ## s(SampleDepth,RelativeDepth) 59 43.23949 1.015062 0.8275 We should also verify the residual plots, to verify whether the Tweedie distribution is appropriate: gam.check(smooth_interact_tw) ## ## Method: REML Optimizer: outer newton ## full convergence after 3 iterations. ## Gradient range [-0.0007646676,0.0001600484] ## (score 1791.138 &amp; scale 0.4355231). ## Hessian positive definite, eigenvalue range [12.02444,492.4714]. ## Model rank = 61 / 61 ## ## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may ## indicate that k is too low, especially if edf is close to k&#39;. ## ## k&#39; edf k-index p-value ## s(SampleDepth,RelativeDepth) 59.0 43.2 1.02 0.83 The residual plots do look much better, but it is clear that something is missing from the model. This could be a spatial affect (longtitude and latitude), or a random effect (e.g. based on Station). 3. Compare smooth_interact_tw with smooth_interact. Which one would you choose? AIC(smooth_interact, smooth_interact_tw) ## df AIC ## smooth_interact 49.47221 4900.567 ## smooth_interact_tw 47.86913 3498.490 AIC allows us to compare models that are based on different distributions The AIC score for smooth_interact_tw is much smaller than the AIC score for the smooth_interact. Using a Tweedie instead of a Normal distribution greatly improves our model! "],["changing-the-basis-function.html", "Chapter 10 Changing the basis function", " Chapter 10 Changing the basis function We won’t go too much further into this in this workshop, but you should know you can expand on the basic model we’ve looked at today with: more complicated smooths, by changing the basis, other distributions: anything you can do with a GLM using the family argument, mixed effect models, using gamm or the gamm4 package. We will first have a look at changing the basis, followed by a quick intro to using other distribution and GAMMs (Generalized Additive Mixed Models). Let’s look at one place where knowing how to change the basis can help: cyclical data. That is, data where you want the predictor to match at the ends. Imagine you have a time series of climate data, broken down by monthly measurement, and you want to see if there’s a time-trend in yearly temperature. We’ll use the Nottingham temperature time series for this: data(nottem) n_years &lt;- length(nottem)/12 nottem_month &lt;- rep(1:12, times = n_years) nottem_year &lt;- rep(1920:(1920 + n_years - 1), each = 12) nottem_plot &lt;- qplot(nottem_month, nottem, colour = factor(nottem_year), geom = &quot;line&quot;) + theme_bw() print(nottem_plot) Using the nottem data, we have created three new vectors; n_years corresponds to the number of years of data (20 years), nottem_month is a categorical variable coding for the 12 months of the year, for every year sampled (sequence 1 to 12 repeated 20 times), and nottem_year is a variable where the year corresponding to each month is provided. Monthly data from the years 1920 through to 1940: To model this, we need to use what’s called a cyclical cubic spline, or \"cc\", to model month effects as well as a term for year. year_gam &lt;- gam(nottem ~ s(nottem_year) + s(nottem_month, bs = &quot;cc&quot;)) summary(year_gam)$s.table plot(year_gam, page = 1, scale = 0) There is about 1-1.5 degree rise in temperature over the period, but within a given year there is about 20 degrees variation in temperature, on average. The actual data vary around these values and that is the unexplained variance. Here we can see one of the very interesting bonuses of using GAMs. We can either plot the response surface (fitted values) or the terms (contribution of each covariate) as shown here. You can imagine these as plots of the changing regression coefficients, and how their contribution (or effect size) varies over time. In the first plot, we see that positive contributions of temperature occurred post-1930. Over longer time scales, for example using paleolimnological data, others (Simpson &amp; Anderson 2009; Fig. 3c) have used GAMs to plot the contribution (effect) of temperature on algal assemblages in lakes, to illustrate how significant contributions only occurred during two extreme cold events (that is, the contribution is significant when the confidence intervals do not overlap zero, at around 300 and 100 B.C.). This allowed the authors to not only determine how much variance was explained by temperature over the last few centuries, but also pinpoint when in time this effect was significant. If of interest to you, the code to plot either the response surface (type = \"response\") or the terms (type = \"terms\") is given below. When terms is selected, you obtain the same figure as above. Contribution plot: pred &lt;- predict(year_gam, type = &quot;terms&quot;, se = TRUE) I &lt;- order(nottem_year) plusCI &lt;- I(pred$fit[, 1] + 1.96 * pred$se[, 1]) minusCI &lt;- I(pred$fit[, 1] - 1.96 * pred$se[, 1]) xx &lt;- c(nottem_year[I], rev(nottem_year[I])) yy &lt;- c(plusCI[I], rev(minusCI[I])) plot(xx, yy, type = &quot;n&quot;, cex.axis = 1.2) polygon(xx, yy, col = &quot;light grey&quot;, border = &quot;light grey&quot;) lines(nottem_year[I], pred$fit[, 1][I], lty = 1) abline(h = 0) "],["other-distributions-1.html", "Chapter 11 Other distributions 11.1 Visualizing the trend over time", " Chapter 11 Other distributions To provide a brief overview on how to use GAMs when the response variable does not follow a normal distributions and is either count or proportion data (e.g., Gamma, binomial, Poisson, negative binomial), the example that follows uses a dataset where a binomial family distribution is needed and a non-linear relationship with the explanatory variable is evident. Here, the response variable represents the number of successes (event happened) versus failures over the course of an experiment. gam_data3 &lt;- read.csv(&quot;other_dist.csv&quot;) summary(gam_data3) str(gam_data3) &#39;data.frame&#39;: 514 obs. of 4 variables: $ prop : num 1 1 1 1 0 1 1 1 1 1 ... $ total: int 4 20 20 18 18 18 20 20 20 20 ... $ x1 : int 550 650 750 850 950 650 750 850 950 550 ... $ fac : Factor w/ 4 levels &quot;f1&quot;,&quot;f2&quot;,&quot;f3&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... prop is the response variable, and is equal to the proportion of successes / (successes + failures). Note that there are numerous cases where the proportion equals 1 or 0 which indicates that the outcomes were always successes or failures, respectively, at a given time point in the experiment. x1 is the time since the start of experiment (our explanatory variable). total represents the number of successes + failures observed at any time point of the experiment. fac is a factor coding for trials 1 through 4 of the experiment (we will not use this variable in this section). Let’s start by visualizing the data. We are interested in the number of successes in comparison to failures as x1 increases. We will calculate the grand averages of the proportions of successes per time bin (x1) given that there are repeated measures per x1 value (numerous trials and observations per trial). emptyPlot(range(gam_data3$x1), c(0, 1), h = 0.5, main = &quot;Probability of successes&quot;, ylab = &quot;Probability&quot;, xlab = &quot;x1&quot;) avg &lt;- aggregate(prop ~ x1, data = gam_data3, mean, na.rm = TRUE) lines(avg$x1, avg$prop, col = &quot;orange&quot;, lwd = 2) As x1 increases, so does the probability of successes. Would you say that this trend is linear or non-linear? We will test this using a logistic GAM (we use a binomial family distribution given that our response is proportion data). prop_model &lt;- gam(prop ~ s(x1), data = gam_data3, weights = total, family = &quot;binomial&quot;) prop_summary &lt;- summary(prop_model) print(prop_summary$p.table) print(prop_summary$s.table) plot(prop_model) Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.173978 0.02709613 43.32641 0 edf Ref.df Chi.sq p-value s(x1) 4.591542 5.615235 798.9407 2.027751e-164 What does the intercept represent in this model? Recall that the model uses the count outcomes to calculate the logit, which is the log odds ratio between successes and failures: \\[logit = log(\\frac{N_{success}}{N_{failures}})\\] If successes = failures, the ratio is 1 and the logit is 0 (i.e., log(1) = 0). If successes have a larger count than failures, the ratio is greater than 1 and the logit has a positive value (e.g., log(2) = 0.69). If successes have a smaller count than failures, the ratio is lower than 1 and the logit has a negative value (e.g., log(0.5) = -0.69). Thus, the intercept is the log odds ratio of successes to failures (logit), and indicates whether on average there are more successes than failures. Here, the estimated intercept coefficient is positive, which means that there are more successes than failures overall. What does the smooth term indicate? This represents how the log odds of successes vs failures changes over x1 (time in this example). So, we see that since the edf&gt;1, the proportion of successes increases faster over time. If, for example, the response represents the count of species A versus species B and nutrient concentrations are increased over time, these results would indicate that species A is increasingly observed as nutrient concentrations approach its niche optima over the course of the experiment. 11.1 Visualizing the trend over time Lastly, we will see the different ways this relationship could be represented graphically. par(mfrow = c(1, 2)) plot(prop_model, select = 1, scale = 0, shade = TRUE) abline(h = 0) out &lt;- plot_smooth(prop_model, view = &quot;x1&quot;, main = &quot;&quot;) (diff &lt;- find_difference(out$fv$fit, out$fv$CI, xVals = out$fv$x1)) addInterval(0, lowVals = diff$start, highVals = diff$end, col = &quot;red&quot;, lwd = 2) abline(v = c(diff$start, diff$end), lty = 3, col = &quot;red&quot;) text(mean(c(diff$start, diff$end)), 2.1, &quot;sign. more \\n success&quot;, col = &quot;red&quot;, font = 3) What do these plots tell us about successes versus failures? Left plot: contribution/ partial effect (if we had more than one explanatory variable). Over time the log odds increases, so over time successes increase and failures decrease. Right plot: fitted values, intercept included (summed effect if we had more than one explanatory variable included in the model). Here we see that the log odds ratio is estimated around zero at the start of the experiment; this means there are equal amounts of successes and failures. Gradually successes increase, and at around x1 = 400 there are significantly more successes than failures (the smooth is significantly different from zero). We also illustrated how we could use the plot to determine at which x1 value this occurs. Lastly, to help interpret the results, we could transform the summed effects back to proportions with the function plot_smooth from the itsadug package: par(mfrow = c(1, 1)) plot_smooth(prop_model, view = &quot;x1&quot;, main = &quot;&quot;, transform = plogis, ylim = c(0, 1)) abline(h = 0.5, v = diff$start, col = &quot;red&quot;, lty = 2) As we already derived from the logit plot, we see that at around x1 = 400 the proportion of successes increases significantly above 0.5. "],["quick-intro-to-generalized-additive-mixed-models-gamms.html", "Chapter 12 Quick intro to Generalized Additive Mixed Models (GAMMs) 12.1 Dealing with non-independence 12.2 Mixed modelling", " Chapter 12 Quick intro to Generalized Additive Mixed Models (GAMMs) 12.1 Dealing with non-independence When observations are not independent, GAMs can be used to either incorporate: a serial correlation structure to model residual autocorrelation (autoregressive: AR; moving average: MA; or a combination of the two: ARMA), random effects that model independence among observations from the same site. That is, in addition to changing the basis as with the nottem example above, we can also add complexity to the model by incorporating an autocorrelation structure or mixed effects using the gamm() function in the mgcv package. Although we will not be using it here, the gamm4 package can also be used to estimate GAMMs in R. To start, let’s have a look at the first case; a model with temporal autocorrelation in the residuals. We will revisit the Nottingham temperature model and test for correlated errors using the (partial) autocorrelation function. par(mfrow = c(1, 2)) acf(resid(year_gam), lag.max = 36, main = &quot;ACF&quot;) pacf(resid(year_gam), lag.max = 36, main = &quot;pACF&quot;) The autocorrelation function plot (ACF; first panel above) provides the cross-correlation of a time series with itself at different points in time (i.e. similarity between observations at increasingly large time lags). In contrast, the partial autocorrelation function (PACF: second panel above) gives the partial correlation of a time series with its own lagged values after controlling for the values of the time series at all shorter lags. The ACF and pACF plots are thus used to identify after how many time steps do observations start to be independent from one another. The ACF plot of our model residuals suggests a significant lag of 1, and perhaps a lag of 2. Therefore, a low-order AR model is likely needed. We can test this by adding AR structures to the Nottingham temperature model, one with an AR(1) (correlation at 1 time step) and one with an AR(2) (correlation at two times steps), and test for the best-fit model using ANOVA. year_gam &lt;- gamm(nottem ~ s(nottem_year) + s(nottem_month, bs = &quot;cc&quot;)) year_gam_AR1 &lt;- gamm(nottem ~ s(nottem_year) + s(nottem_month, bs = &quot;cc&quot;), correlation = corARMA(form = ~1 | nottem_year, p = 1)) year_gam_AR2 &lt;- gamm(nottem ~ s(nottem_year) + s(nottem_month, bs = &quot;cc&quot;), correlation = corARMA(form = ~1 | nottem_year, p = 2)) anova(year_gam$lme, year_gam_AR1$lme, year_gam_AR2$lme) Model df AIC BIC logLik Test L.Ratio p-value year_gam$lme 1 5 1109.908 1127.311 -549.9538 year_gam_AR1$lme 2 6 1101.218 1122.102 -544.6092 1 vs 2 10.689206 0.0011 year_gam_AR2$lme 3 7 1101.598 1125.962 -543.7988 2 vs 3 1.620821 0.2030 The AR(1) provides a significant increase in fit over the naive model (LRT = 10.69, p = 0.0011), but there is very little improvement in moving to the AR(2) (LRT = 1.62, p = 0.203). So it is best to include only the AR(1) structure in our model. 12.2 Mixed modelling As we saw in the previous section, bs specifies the type of underlying base function. For random intercepts and linear random slopes we use bs=\\\"re\\\", but for random smooths we use bs=\\\"fs\\\". Three different types of random effects are distinguished when using GAMMs (where fac is a categorial variable coding for the random effect and x0 is a continuous fixed effect): random intercepts adjust the height of other model terms with a constant value: s(fac, bs=\\\"re\\\") random slopes adjust the slope of the trend of a numeric predictor: s(fac, x0, bs=\\\"re\\\") random smooths adjust the trend of a numeric predictor in a nonlinear way: s(x0, factor, bs=\\\"fs\\\", m=1), where the argument m=1 sets a heavier penalty for the smooth moving away from 0, causing shrinkage to the mean. We will first examine a GAMM with only a random intercept. As before, we will use the gamSim() function to automatically generate a dataset, this time with a random effect component generate, then run a model with a random intercept using fac as the random factor. # generate and view data gam_data2 &lt;- gamSim(eg = 6) head(gam_data2) # run random intercept model gamm_intercept &lt;- gam(y ~ s(x0) + s(fac, bs = &quot;re&quot;), data = gam_data2) # examine model output summary(gamm_intercept)$s.table Note that there is now a smoother term for the random intercept in the summary table. You can plot and view the random intercepts for each level of fac as follows: plot(gamm_intercept, select = 2) # select=2 because the random effect appears as the second # entry in the summary table. We can also use the plot_smooth function to visualize the model, which in contrast to the default plot.gam, allows us to plot a smooth of the summed effects of a GAM (based on predictions) as we saw earlier, but in addition, optionally removes the random effects. Here, we will plot the summed effects for the x0 without random effects, and then plot the predictions of all four levels of the random fac effect: par(mfrow = c(1, 2), cex = 1.1) plot_smooth(gamm_intercept, view = &quot;x0&quot;, rm.ranef = TRUE, main = &quot;intercept + s(x1)&quot;, rug = FALSE) plot_smooth(gamm_intercept, view = &quot;x0&quot;, cond = list(fac = &quot;1&quot;), main = &quot;... + s(fac)&quot;, col = &quot;orange&quot;, ylim = c(8, 21), rug = FALSE) plot_smooth(gamm_intercept, view = &quot;x0&quot;, cond = list(fac = &quot;2&quot;), add = TRUE, col = &quot;red&quot;) plot_smooth(gamm_intercept, view = &quot;x0&quot;, cond = list(fac = &quot;3&quot;), add = TRUE, col = &quot;purple&quot;) plot_smooth(gamm_intercept, view = &quot;x0&quot;, cond = list(fac = &quot;4&quot;), add = TRUE, col = &quot;turquoise&quot;) Next we will run and plot a model with random slopes: gamm_slope &lt;- gam(y ~ s(x0) + s(x0, fac, bs = &quot;re&quot;), data = gam_data2) summary(gamm_slope)$s.table plot_smooth(gamm_slope, view = &quot;x0&quot;, rm.ranef = TRUE, main = &quot;intercept + s(x0)&quot;, rug = FALSE) plot_smooth(gamm_slope, view = &quot;x0&quot;, cond = list(fac = &quot;1&quot;), main = &quot;... + s(fac)&quot;, col = &quot;orange&quot;, ylim = c(7, 22), rug = FALSE) plot_smooth(gamm_slope, view = &quot;x0&quot;, cond = list(fac = &quot;2&quot;), add = TRUE, col = &quot;red&quot;) plot_smooth(gamm_slope, view = &quot;x0&quot;, cond = list(fac = &quot;3&quot;), add = TRUE, col = &quot;purple&quot;) plot_smooth(gamm_slope, view = &quot;x0&quot;, cond = list(fac = &quot;4&quot;), add = TRUE, col = &quot;turquoise&quot;) We will now include both a random intercept and slope term. gamm_int_slope &lt;- gam(y ~ s(x0) + s(fac, bs = &quot;re&quot;) + s(fac, x0, bs = &quot;re&quot;), data = gam_data2) summary(gamm_int_slope)$s.table plot_smooth(gamm_int_slope, view = &quot;x0&quot;, rm.ranef = TRUE, main = &quot;intercept + s(x0)&quot;, rug = FALSE) plot_smooth(gamm_int_slope, view = &quot;x0&quot;, cond = list(fac = &quot;1&quot;), main = &quot;... + s(fac) + s(fac, x0)&quot;, col = &quot;orange&quot;, ylim = c(7, 22), rug = FALSE) plot_smooth(gamm_int_slope, view = &quot;x0&quot;, cond = list(fac = &quot;2&quot;), add = TRUE, col = &quot;red&quot;, xpd = TRUE) plot_smooth(gamm_int_slope, view = &quot;x0&quot;, cond = list(fac = &quot;3&quot;), add = TRUE, col = &quot;purple&quot;, xpd = TRUE) plot_smooth(gamm_int_slope, view = &quot;x0&quot;, cond = list(fac = &quot;4&quot;), add = TRUE, col = &quot;turquoise&quot;, xpd = TRUE) Note that the random slope is static in this case: plot(gamm_int_slope, select = 3) # select=3 because the random slope appears as the third # entry in your summary table. Lastly, we will examine a model with a random smooth. gamm_smooth &lt;- gam(y ~ s(x0, fac, bs = &quot;fs&quot;, m = 1), data = gam_data2) summary(gamm_smooth)$s.table Here, if the random slope varied along x0, we would see different curves for each level: plot(gamm_smooth, select = 1) # select=1 because the smooth slope appears as the first # entry in your summary table. All of the above models can in turn be compared using anova() as in the previous sections to determine the best fit model. "],["resources.html", "Chapter 13 Resources", " Chapter 13 Resources This workshop is intended as a brief introduction to basic concepts, and popular packages to help you estimate, evaluate, and visualise GAMs in R, but there is so much more to GAMs! We have a few resources to suggest if you would like to delve deeper into the subject of GAMs and how to implement them in R. Many of these resources inspired and contributed to the contents of this workshop. This is not meant to be an exhaustive list, but provides some very helpful next steps. The book Generalized Additive Models: An Introduction with R by Simon Wood (the author of the mgcv package) is probably the most thorough resource you can find about GAMs. Gavin Simpson’s blog, From the bottom of the heap, covers a lot of different aspects of GAMs and how to implement them in R. Gavin Simpson’s package gratia is a useful reimplementation of GAM visualisation tools in ggplot2. Generalized Additive Models: An Introduction with R by Noam Ross is well-designed, interactive, and free course that covers GAMs in greater detail. Overview GAMM analysis of time series data by Jacolien van Rij is a helpful in-depth tutorial about GAMMs that greatly inspired the GAMM section of this workshop. Simon Wood also catalogues talks and notes about GAMs on his website (maths.ed.ac.uk/~swood34/). Hierarchical generalized additive models in ecology: an introduction with mgcv by Pedersen et al. (2019) is a great introduction to hierarchical GAMs, how to design them, and how they can be implemented in R. Finally, the help pages, available through ?gam in R, are always an excellent resource. "],["references.html", "Chapter 14 References", " Chapter 14 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
